
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3. Regression &#8212; Coding for Economists</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="canonical" href="https://aeturrell.github.io/coding-for-economists/econmt-regression.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="1. Intro to Geo-Spatial Analysis" href="geo-intro.html" />
    <link rel="prev" title="2. Statistics" href="econmt-statistics.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://aeturrell.github.io/coding-for-economists/econmt-regression.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Regression" />
<meta property="og:description" content="Regression  Introduction  In this chapter, you’ll learn how to run linear regressions with code.  If you’re running this code (either by copying and pasting it," />
<meta property="og:image"       content="https://aeturrell.github.io/coding-for-economists/_static/smith_lovelace.png" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/smith_lovelace.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Coding for Economists</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Basics
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="basics-preliminaries.html">
   1. Preliminaries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="basics-of-coding.html">
   2. Basics of Coding
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Data
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="data-analysis-quickstart.html">
   1. Data Analysis Quickstart
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data-intro.html">
   2. Data Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data-read-and-write.html">
   3. Reading and Writing Files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data-extraction.html">
   4. Extracting Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data-advanced.html">
   5. Advanced Data
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Data Visualisation
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="vis-intro.html">
   1. Intro to Data Visualisation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="vis-common-plots.html">
   2. Common Plots
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Best practice
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="best-practice-coding.html">
   1. Best Practice in Coding
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Mathematics with Code
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="maths-maths-coding.html">
   1. Intro to Mathematics with Code
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Econometrics
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="econmt-probability-random.html">
   1. Probability and Random Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="econmt-statistics.html">
   2. Statistics
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Regression
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Geospatial
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="geo-intro.html">
   1. Intro to Geo-Spatial Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="geo-vis.html">
   2. Geo-Spatial Visualisation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Coming from ...
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="coming-from-stata.html">
   Coming from Stata
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="coming-from-matlab.html">
   Coming from Matlab
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="coming-from-r.html">
   Coming from R
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Elsewhere
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="zreferences.html">
   Bibliography
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/econmt-regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Feconmt-regression.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/econmt-regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/executablebooks/jupyter-book/blob/master/docs/econmt-regression.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   3.1. Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notation-and-basic-definitions">
     3.1.1. Notation and basic definitions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#imports">
     3.1.2. Imports
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regression-basics">
   3.2. Regression basics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#robust-regression">
     3.2.1. Robust regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#standard-errors">
     3.2.2. Standard errors
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#clustered-standard-errors">
       3.2.2.1. Clustered standard errors
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fixed-effects-and-categorical-variables">
   3.3. Fixed effects and categorical variables
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#transformations-of-regressors">
   3.4. Transformations of regressors
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logs-and-arcsinh">
     3.4.1. Logs and arcsinh
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interaction-terms-and-powers">
     3.4.2. Interaction terms and powers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-formula-api-explained">
   3.5. The formula API explained
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiple-regression-models">
   3.6. Multiple regression models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#specifying-regressions-without-formulae-using-the-array-api">
   3.7. Specifying regressions without formulae, using the array API
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fixed-effects-in-the-array-api">
     3.7.1. Fixed effects in the array API
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#instrumental-variables">
   3.8. Instrumental variables
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logit-probit-and-generalised-linear-models">
   3.9. Logit, probit, and generalised linear models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logit">
     3.9.1. Logit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probit">
     3.9.2. Probit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalised-linear-models">
     3.9.3. Generalised linear models
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quantile-regression">
   3.10. Quantile regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regression-plots">
   3.11. Regression plots
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#specification-curve-analysis">
   3.12. Specification curve analysis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#review">
   3.13. Review
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="regression">
<h1><span class="section-number">3. </span>Regression<a class="headerlink" href="#regression" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2><span class="section-number">3.1. </span>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>In this chapter, you’ll learn how to run linear regressions with code.</p>
<p>If you’re running this code (either by copying and pasting it, or by downloading it using the icons at the top of the page), you may need to the packages it uses by, for example, running <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">packagename</span></code> on your computer’s command line. (If you’re not sure what a command line is, take a quick look at the basics of coding chapter.)</p>
<p>Most of this chapter will rely on <a class="reference external" href="https://www.statsmodels.org/stable/index.html">statsmodels</a> with some use of <a class="reference external" href="https://bashtage.github.io/linearmodels/"><strong>linearmodels</strong></a>. Some of the material in this chapter follows <a class="reference external" href="https://grantmcdermott.com/">Grant McDermott</a>’s excellent notes and the <a class="reference external" href="https://lost-stats.github.io/">Library of Statistical Translation</a>.</p>
<div class="section" id="notation-and-basic-definitions">
<h3><span class="section-number">3.1.1. </span>Notation and basic definitions<a class="headerlink" href="#notation-and-basic-definitions" title="Permalink to this headline">¶</a></h3>
<p>Greek letters, like <span class="math notranslate nohighlight">\(\beta\)</span>, are the truth and represent parameters. Modified Greek letters are an estimate of the truth, for example <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>. Sometimes Greek letters will stand in for vectors of parameters. Most of the time, upper case Latin characters such as <span class="math notranslate nohighlight">\(X\)</span> will represent random variables (which could have more than one dimension). Lower case letters from the Latin alphabet denote realised data, for instance <span class="math notranslate nohighlight">\(x\)</span> (which again could be multi-dimensional).  Modified Latin alphabet letters denote computations performed on data, for instance <span class="math notranslate nohighlight">\(\bar{x} = \frac{1}{n} \displaystyle\sum_{i} x_i\)</span> where <span class="math notranslate nohighlight">\(n\)</span> is number of samples.</p>
<p>Ordinary least squares (OLS) regression is used to predict the value of an outcome variable <span class="math notranslate nohighlight">\(y\)</span> based on one or more input predictor variables arrange in a matrix <span class="math notranslate nohighlight">\(X\)</span>. The equation that we would like to recover is</p>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2
\]</div>
<p>where the <span class="math notranslate nohighlight">\(x_i\)</span> could be transforms of original data. But this is a platonic ideal, what is called the data generating process (DGP). All we can do is recover <em>estimates</em> of it, i.e. we can find <span class="math notranslate nohighlight">\(\hat{\beta_i}\)</span> and the relationship</p>
<div class="math notranslate nohighlight">
\[
y = \hat{\beta_0} + \hat{\beta_1} \cdot x_1 + \hat{\beta_2} \cdot x_2 + \epsilon
\]</div>
<p>This equation can also be expressed in matrix form as</p>
<div class="math notranslate nohighlight">
\[
y = x'\cdot \hat{\beta} + \epsilon
\]</div>
<p>where <span class="math notranslate nohighlight">\(x' = (1, x_1, \dots, x_{n})'\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta} = (\hat{\beta_0}, \hat{\beta_1}, \dots, \hat{\beta_{n}})\)</span>.</p>
<p>Given data <span class="math notranslate nohighlight">\(y_i\)</span> stacked to make a vector <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(x_{i}\)</span> stacked to make a matrix <span class="math notranslate nohighlight">\(X\)</span>, this can be solved for the coefficients <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> according to</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta} = \left(X'X\right)^{-1} X'y
\]</div>
<p>The interpretation of these regression coefficients depends on what their units are to begin with, but you can always work it out by differentiating both sides of the model equation with respect to the <span class="math notranslate nohighlight">\(x_i\)</span>. For example, for the first model equation above</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial y}{\partial x_i} = \beta_i
\]</div>
<p>so we get the interpretation that <span class="math notranslate nohighlight">\(\beta_i\)</span> is the rate of change of y with respect to <span class="math notranslate nohighlight">\(x_i\)</span>. If <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are in levels, this means that a unit increase in <span class="math notranslate nohighlight">\(x_i\)</span> is associated with a <span class="math notranslate nohighlight">\(\beta_i\)</span> units increase in <span class="math notranslate nohighlight">\(y\)</span>. If the right-hand side of the model is <span class="math notranslate nohighlight">\(\ln x_i\)</span> then we get</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial y}{\partial x_i} = \beta_i \frac{1}{x_i} 
\]</div>
<p>with some abuse of notation, we can rewrite this as <span class="math notranslate nohighlight">\(dy = \beta_i dx_i/x_i\)</span>, which says that a percent change in <span class="math notranslate nohighlight">\(x_i\)</span> is associated with a <span class="math notranslate nohighlight">\(\beta_i\)</span> unit change in <span class="math notranslate nohighlight">\(y\)</span>. With a logged <span class="math notranslate nohighlight">\(y\)</span> variable, it’s a percent change in <span class="math notranslate nohighlight">\(x_i\)</span> that is associated with a percent change in <span class="math notranslate nohighlight">\(y\)</span>, or <span class="math notranslate nohighlight">\(dy/y = \beta_i dx_i/x_i\)</span> (note that both sides of this equation are unitless in this case). Finally, another example that is important in practice is that of log differences, eg <span class="math notranslate nohighlight">\(y = \beta_i (\ln x_i - \ln x_i')\)</span>. Again, we will abuse notation and say that this case may be represented as <span class="math notranslate nohighlight">\(dy = \beta_i (d x_i/x_i - dx_i'/x_i')\)</span>, i.e. the difference in two percentages, a <em>percentage point</em> change, in <span class="math notranslate nohighlight">\(x_i\)</span> is associated with a <span class="math notranslate nohighlight">\(\beta_i\)</span> unit change in <span class="math notranslate nohighlight">\(y\)</span>.</p>
</div>
<div class="section" id="imports">
<h3><span class="section-number">3.1.2. </span>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">¶</a></h3>
<p>Let’s import some of the packages we’ll be using:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">st</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
<span class="kn">import</span> <span class="nn">os</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set max rows displayed for readability</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.max_rows&#39;</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="c1"># Plot settings</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;plot_style.txt&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="regression-basics">
<h2><span class="section-number">3.2. </span>Regression basics<a class="headerlink" href="#regression-basics" title="Permalink to this headline">¶</a></h2>
<p>There are two ways to run regressions in <a class="reference external" href="https://www.statsmodels.org/stable/index.html"><strong>statsmodels</strong></a>; passing the data directly as objects, and using formulae. We’ll see both but, just to get things started, let’s use the formula API.</p>
<p>We’ll use the starwars dataset to run a regression of mass on height for star wars characters. First, let’s bring the dataset in:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;starwars.pickle&#39;</span><span class="p">))</span>
<span class="c1"># Look at first few rows</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>height</th>
      <th>mass</th>
      <th>hair_color</th>
      <th>eye_color</th>
      <th>gender</th>
      <th>homeworld</th>
      <th>species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Luke Skywalker</td>
      <td>172.0</td>
      <td>77.0</td>
      <td>blond</td>
      <td>blue</td>
      <td>male</td>
      <td>Tatooine</td>
      <td>Human</td>
    </tr>
    <tr>
      <th>1</th>
      <td>C-3PO</td>
      <td>167.0</td>
      <td>75.0</td>
      <td>NaN</td>
      <td>yellow</td>
      <td>NaN</td>
      <td>Tatooine</td>
      <td>Droid</td>
    </tr>
    <tr>
      <th>2</th>
      <td>R2-D2</td>
      <td>96.0</td>
      <td>32.0</td>
      <td>NaN</td>
      <td>red</td>
      <td>NaN</td>
      <td>Naboo</td>
      <td>Droid</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Darth Vader</td>
      <td>202.0</td>
      <td>136.0</td>
      <td>none</td>
      <td>yellow</td>
      <td>male</td>
      <td>Tatooine</td>
      <td>Human</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Leia Organa</td>
      <td>150.0</td>
      <td>49.0</td>
      <td>brown</td>
      <td>brown</td>
      <td>female</td>
      <td>Alderaan</td>
      <td>Human</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Okay, now let’s do a regression using OLS and a formula that says our y-variable is mass and our regressor is height:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mass ~ height&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Well, where are the results!? They’re stored in the object we created. To peek at them we need to call the summary function (and, for easy reading, I’ll print it out too using <code class="docutils literal notranslate"><span class="pre">print</span></code>)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   mass   R-squared:                       0.018
Model:                            OLS   Adj. R-squared:                  0.001
Method:                 Least Squares   F-statistic:                     1.040
Date:                Sun, 28 Feb 2021   Prob (F-statistic):              0.312
Time:                        13:26:57   Log-Likelihood:                -385.50
No. Observations:                  59   AIC:                             775.0
Df Residuals:                      57   BIC:                             779.2
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept    -13.8103    111.155     -0.124      0.902    -236.393     208.773
height         0.6386      0.626      1.020      0.312      -0.615       1.892
==============================================================================
Omnibus:                      128.880   Durbin-Watson:                   2.025
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             7330.447
Skew:                           7.340   Prob(JB):                         0.00
Kurtosis:                      55.596   Cond. No.                         895.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>What we’re seeing here are really several tables glued together. To just grab the coefficients in a tidy format, use</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>  -13.8103</td> <td>  111.155</td> <td>   -0.124</td> <td> 0.902</td> <td> -236.393</td> <td>  208.773</td>
</tr>
<tr>
  <th>height</th>    <td>    0.6386</td> <td>    0.626</td> <td>    1.020</td> <td> 0.312</td> <td>   -0.615</td> <td>    1.892</td>
</tr>
</table></div></div>
</div>
<p>You’ll have noticed that we got an intercept, even though we didn’t specify one in the formula. <strong>statsmodels</strong> adds in an intercept by default because, most of the time, you will want one. To turn it off, add a <code class="docutils literal notranslate"><span class="pre">-1</span></code> at the end of the formula command, eg in this case you would call <code class="docutils literal notranslate"><span class="pre">smf.ols('mass</span> <span class="pre">~</span> <span class="pre">height</span> <span class="pre">-1',</span> <span class="pre">data=df).fit()</span></code>.</p>
<p>The fit we got in the case with the intercept was pretty terrible; a low <span class="math notranslate nohighlight">\(R^2\)</span> and both of our confidence intervals are large and contain zero. What’s going on? If there’s one adage in regression that’s always worth paying attention to, it’s <em>always plot your data</em>. Let’s see what’s going on here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;mass&quot;</span><span class="p">,</span>
                <span class="n">s</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Jabba the Hutt&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;mass&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()][[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;mass&#39;</span><span class="p">]],</span>
            <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">50</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span>
            <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">&quot;fancy&quot;</span><span class="p">,</span>
                            <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span>
                            <span class="n">connectionstyle</span><span class="o">=</span><span class="s2">&quot;arc3,rad=0.3&quot;</span><span class="p">,</span>
                            <span class="p">)</span>
            <span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Always look for outliers!&#39;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/econmt-regression_13_0.png" src="_images/econmt-regression_13_0.png" />
</div>
</div>
<p>Oh dear, Jabba’s been on the paddy frogs again, and he’s a bit of different case. When we’re estimating statistical relationships, we have all kinds of choices and should be wary about arbitrary decisions of what to include or exclude in case we fool ourselves about the generality of the relationship we are capturing. Let’s say we knew that we weren’t interested in Hutts though, but only in other species: in that case, it’s fair enough to filter out Jabba and run the regression without this obvious outlier. We’ll exclude any entry that contains the string ‘Jabba’ in the <code class="docutils literal notranslate"><span class="pre">name</span></code> column:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_outlier_free</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mass ~ height&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="o">~</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;Jabba&#39;</span><span class="p">)])</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_outlier_free</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   mass   R-squared:                       0.580
Model:                            OLS   Adj. R-squared:                  0.572
Method:                 Least Squares   F-statistic:                     77.18
Date:                Sun, 28 Feb 2021   Prob (F-statistic):           4.02e-12
Time:                        13:26:57   Log-Likelihood:                -252.48
No. Observations:                  58   AIC:                             509.0
Df Residuals:                      56   BIC:                             513.1
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept    -32.5408     12.561     -2.591      0.012     -57.703      -7.379
height         0.6214      0.071      8.785      0.000       0.480       0.763
==============================================================================
Omnibus:                        9.951   Durbin-Watson:                   1.657
Prob(Omnibus):                  0.007   Jarque-Bera (JB):               10.072
Skew:                           0.793   Prob(JB):                      0.00650
Kurtosis:                       4.285   Cond. No.                         888.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>This looks a lot more healthy. Not only is the model explaining a <em>lot</em> more of the data, but the coefficients are now significant.</p>
<div class="section" id="robust-regression">
<h3><span class="section-number">3.2.1. </span>Robust regression<a class="headerlink" href="#robust-regression" title="Permalink to this headline">¶</a></h3>
<p>Filtering out data is one way to deal with outliers, but it’s not the only one; an alternative is to use a regression technique that is robust to such outliers. <strong>statsmodels</strong> has a variety of robust linear models that you can read more about <a class="reference external" href="https://www.statsmodels.org/stable/examples/notebooks/generated/robust_models_0.html">here</a>. To demonstrate the general idea, we will run the regression again but using a robust method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_robust</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">rlm</span><span class="p">(</span><span class="s1">&#39;mass ~ height&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">robust</span><span class="o">.</span><span class="n">norms</span><span class="o">.</span><span class="n">TrimmedMean</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_robust</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                    Robust linear Model Regression Results                    
==============================================================================
Dep. Variable:                   mass   No. Observations:                   59
Model:                            RLM   Df Residuals:                       57
Method:                          IRLS   Df Model:                            1
Norm:                     TrimmedMean                                         
Scale Est.:                       mad                                         
Cov Type:                          H1                                         
Date:                Sun, 28 Feb 2021                                         
Time:                        13:26:57                                         
No. Iterations:                     7                                         
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept    -31.4968      2.180    -14.451      0.000     -35.769     -27.225
height         0.6273      0.012     51.102      0.000       0.603       0.651
==============================================================================

If the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .
</pre></div>
</div>
</div>
</div>
<p>There are many different ‘M-estimators’ available; in this case the TrimmedMean estimator gives a very similar result to the regression with the point excluded. We can visualise this, and, well, the results are not really very different in this case. Note that <code class="docutils literal notranslate"><span class="pre">abline_plot</span></code> just takes an intercept and coefficient from a fitted model and renders the line that they encode.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;mass&#39;</span><span class="p">])</span>
<span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">abline_plot</span><span class="p">(</span><span class="n">model_results</span><span class="o">=</span><span class="n">results_robust</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Robust&#39;</span><span class="p">)</span>
<span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">abline_plot</span><span class="p">(</span><span class="n">model_results</span><span class="o">=</span><span class="n">results</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;OLS&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Height&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Mass&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/econmt-regression_20_0.png" src="_images/econmt-regression_20_0.png" />
</div>
</div>
</div>
<div class="section" id="standard-errors">
<h3><span class="section-number">3.2.2. </span>Standard errors<a class="headerlink" href="#standard-errors" title="Permalink to this headline">¶</a></h3>
<p>You’ll have seen that there’s a column for the standard error of the estimates in the regression table and a message saying that the covariance type of these is ‘nonrobust’. Let’s say that, instead, we want to use Eicker-Huber-White robust standard errors, aka “HC2” standard errors. We can specify to use these up front standard errors up front in the fit method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mass ~ height&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
    <span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span><span class="o">=</span><span class="s1">&#39;HC2&#39;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">summary</span><span class="p">()</span>
    <span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>  -13.8103</td> <td>   23.456</td> <td>   -0.589</td> <td> 0.556</td> <td>  -59.782</td> <td>   32.162</td>
</tr>
<tr>
  <th>height</th>    <td>    0.6386</td> <td>    0.088</td> <td>    7.263</td> <td> 0.000</td> <td>    0.466</td> <td>    0.811</td>
</tr>
</table></div></div>
</div>
<p>Or, alternatively, we can go back to our existing results and recompute the results from those:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">get_robustcov_results</span><span class="p">(</span><span class="s1">&#39;HC2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   mass   R-squared:                       0.018
Model:                            OLS   Adj. R-squared:                  0.001
Method:                 Least Squares   F-statistic:                     52.75
Date:                Sun, 28 Feb 2021   Prob (F-statistic):           1.16e-09
Time:                        13:26:57   Log-Likelihood:                -385.50
No. Observations:                  59   AIC:                             775.0
Df Residuals:                      57   BIC:                             779.2
Df Model:                           1                                         
Covariance Type:                  HC2                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept    -13.8103     23.456     -0.589      0.558     -60.779      33.159
height         0.6386      0.088      7.263      0.000       0.463       0.815
==============================================================================
Omnibus:                      128.880   Durbin-Watson:                   2.025
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             7330.447
Skew:                           7.340   Prob(JB):                         0.00
Kurtosis:                      55.596   Cond. No.                         895.
==============================================================================

Notes:
[1] Standard Errors are heteroscedasticity robust (HC2)
</pre></div>
</div>
</div>
</div>
<p>There are several different types of standard errors available in <strong>statsmodels</strong>:</p>
<ul class="simple">
<li><p>‘HC0’, ‘HC1’, ‘HC2’, and ‘HC3’</p></li>
<li><p>‘HAC’, for heteroskedasticity and autocorrelation consistent standard errors, for which you may want to also use some keyword arguments</p></li>
<li><p>‘hac-groupsum’, for Driscoll and Kraay heteroscedasticity and
autocorrelation robust standard errors in panel data, again for which you may have to specify extra keyword arguments</p></li>
<li><p>‘hac-panel’, for heteroscedasticity and autocorrelation robust standard
errors in panel data, again with keyword arguments; and</p></li>
<li><p>‘cluster’ for clustered standard errors.</p></li>
</ul>
<p>You can find information on all of these <a class="reference external" href="https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLSResults.get_robustcov_results.html?highlight=get_robustcov_results#statsmodels.regression.linear_model.OLSResults.get_robustcov_results">here</a>. For more on standard errors in python, <a class="reference external" href="http://www.vincentgregoire.com/standard-errors-in-python/">this is a good</a> link.</p>
<p>For now, let’s look more closely at those last ones: clustered standard errors.</p>
<div class="section" id="clustered-standard-errors">
<h4><span class="section-number">3.2.2.1. </span>Clustered standard errors<a class="headerlink" href="#clustered-standard-errors" title="Permalink to this headline">¶</a></h4>
<p>Often, we know something about the structure of likely errors, namely that they occur in groups. In the below example we use one-way clusters to capture this effect in the errors.</p>
<p>Note that in the below example, we grab a subset of the data for which a set of variables we’re interested in are defined, otherwise the below example would execute with an error because of missing cluster-group values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xf</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;homeworld&#39;</span><span class="p">,</span> <span class="s1">&#39;mass&#39;</span><span class="p">,</span> <span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;species&#39;</span><span class="p">])</span>
<span class="n">results_clus</span> <span class="o">=</span> <span class="p">(</span><span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mass ~ height&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">xf</span><span class="p">)</span>
                   <span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span><span class="o">=</span><span class="s1">&#39;cluster&#39;</span><span class="p">,</span> <span class="n">cov_kwds</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;groups&#39;</span><span class="p">:</span> <span class="n">xf</span><span class="p">[</span><span class="s1">&#39;homeworld&#39;</span><span class="p">]}))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_clus</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   mass   R-squared:                       0.014
Model:                            OLS   Adj. R-squared:                 -0.005
Method:                 Least Squares   F-statistic:                     39.44
Date:                Sun, 28 Feb 2021   Prob (F-statistic):           2.63e-07
Time:                        13:26:58   Log-Likelihood:                -361.23
No. Observations:                  55   AIC:                             726.5
Df Residuals:                      53   BIC:                             730.5
Df Model:                           1                                         
Covariance Type:              cluster                                         
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -8.7955     29.150     -0.302      0.763     -65.929      48.338
height         0.6159      0.098      6.280      0.000       0.424       0.808
==============================================================================
Omnibus:                      121.086   Durbin-Watson:                   2.029
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5943.317
Skew:                           7.095   Prob(JB):                         0.00
Kurtosis:                      51.909   Cond. No.                         958.
==============================================================================

Notes:
[1] Standard Errors are robust to cluster correlation (cluster)
</pre></div>
</div>
</div>
</div>
<p>We can add two-way clustering of standard errors using the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xf</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;homeworld&#39;</span><span class="p">,</span> <span class="s1">&#39;mass&#39;</span><span class="p">,</span> <span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;species&#39;</span><span class="p">])</span>
<span class="n">two_way_clusters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">xf</span><span class="p">[[</span><span class="s1">&#39;homeworld&#39;</span><span class="p">,</span> <span class="s1">&#39;species&#39;</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
<span class="n">results_clus</span> <span class="o">=</span> <span class="p">(</span><span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mass ~ height&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">xf</span><span class="p">)</span>
                   <span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span><span class="o">=</span><span class="s1">&#39;cluster&#39;</span><span class="p">,</span>
                        <span class="n">cov_kwds</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;groups&#39;</span><span class="p">:</span> <span class="n">two_way_clusters</span><span class="p">}))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_clus</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   mass   R-squared:                       0.014
Model:                            OLS   Adj. R-squared:                 -0.005
Method:                 Least Squares   F-statistic:                     35.10
Date:                Sun, 28 Feb 2021   Prob (F-statistic):           1.96e-06
Time:                        13:26:58   Log-Likelihood:                -361.23
No. Observations:                  55   AIC:                             726.5
Df Residuals:                      53   BIC:                             730.5
Df Model:                           1                                         
Covariance Type:              cluster                                         
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -8.7955     30.636     -0.287      0.774     -68.841      51.250
height         0.6159      0.104      5.925      0.000       0.412       0.820
==============================================================================
Omnibus:                      121.086   Durbin-Watson:                   2.029
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5943.317
Skew:                           7.095   Prob(JB):                         0.00
Kurtosis:                      51.909   Cond. No.                         958.
==============================================================================

Notes:
[1] Standard Errors are robust to cluster correlation (cluster)
</pre></div>
</div>
</div>
</div>
<p>As you would generally expect, the addition of clustering has increased the standard errors.</p>
</div>
</div>
</div>
<div class="section" id="fixed-effects-and-categorical-variables">
<h2><span class="section-number">3.3. </span>Fixed effects and categorical variables<a class="headerlink" href="#fixed-effects-and-categorical-variables" title="Permalink to this headline">¶</a></h2>
<p>Fixed effects are a way of allowing the intercept of a regression model to vary freely across individuals or groups. It is, for example, used to control for any individual-specific attributes that do not vary across time in panel data.</p>
<p>Let’s use the ‘mtcars’ dataset to demonstrate this. We’ll read it in and set the datatypes of some of the columns at the same time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mpg</span> <span class="o">=</span> <span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/LOST-STATS/lost-stats.github.io/source/Data/mtcars.csv&#39;</span><span class="p">,</span>
                   <span class="n">dtype</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="s1">&#39;mpg&#39;</span><span class="p">:</span><span class="nb">float</span><span class="p">,</span> <span class="s1">&#39;hp&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="s1">&#39;disp&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="s1">&#39;cyl&#39;</span><span class="p">:</span> <span class="s2">&quot;category&quot;</span><span class="p">}))</span>
<span class="n">mpg</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model</th>
      <th>mpg</th>
      <th>cyl</th>
      <th>disp</th>
      <th>hp</th>
      <th>drat</th>
      <th>wt</th>
      <th>qsec</th>
      <th>vs</th>
      <th>am</th>
      <th>gear</th>
      <th>carb</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Mazda RX4</td>
      <td>21.0</td>
      <td>6</td>
      <td>160.0</td>
      <td>110.0</td>
      <td>3.90</td>
      <td>2.620</td>
      <td>16.46</td>
      <td>0</td>
      <td>1</td>
      <td>4</td>
      <td>4</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Mazda RX4 Wag</td>
      <td>21.0</td>
      <td>6</td>
      <td>160.0</td>
      <td>110.0</td>
      <td>3.90</td>
      <td>2.875</td>
      <td>17.02</td>
      <td>0</td>
      <td>1</td>
      <td>4</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Datsun 710</td>
      <td>22.8</td>
      <td>4</td>
      <td>108.0</td>
      <td>93.0</td>
      <td>3.85</td>
      <td>2.320</td>
      <td>18.61</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Hornet 4 Drive</td>
      <td>21.4</td>
      <td>6</td>
      <td>258.0</td>
      <td>110.0</td>
      <td>3.08</td>
      <td>3.215</td>
      <td>19.44</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Hornet Sportabout</td>
      <td>18.7</td>
      <td>8</td>
      <td>360.0</td>
      <td>175.0</td>
      <td>3.15</td>
      <td>3.440</td>
      <td>17.02</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now we have our data in we want to regress mpg (miles per gallon) on hp (horsepower) with fixed effects for cyl (cylinders). Now we <em>could</em> just pop in a formula like this <code class="docutils literal notranslate"><span class="pre">'mpg</span> <span class="pre">~</span> <span class="pre">hp</span> <span class="pre">+</span> <span class="pre">cyl'</span></code> because we took the trouble to declare that <code class="docutils literal notranslate"><span class="pre">cyl</span></code> was of datatype category when reading it in from the csv file. This means that statsmodels will treat it as a category and use it as a fixed effect by default.</p>
<p>But when I read that formula I get nervous that <code class="docutils literal notranslate"><span class="pre">cyl</span></code> might not have been processed correctly (ie it could have been read in as a float, which is what it looks like) and it might just be treated as a float (aka a continuous variable) in the regression. Which is not what we want at all. So, to be safe, and make our intentions explicit (even when the data is of type ‘category’), it’s best to use the syntax <code class="docutils literal notranslate"><span class="pre">C(cyl)</span></code> to ask for a fixed effect.</p>
<p>Here’s a regression which does that:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_fe</span> <span class="o">=</span> <span class="p">(</span><span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mpg ~ hp + C(cyl)&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span>
                 <span class="o">.</span><span class="n">fit</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_fe</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                    mpg   R-squared:                       0.754
Model:                            OLS   Adj. R-squared:                  0.727
Method:                 Least Squares   F-statistic:                     28.59
Date:                Sun, 28 Feb 2021   Prob (F-statistic):           1.14e-08
Time:                        13:26:58   Log-Likelihood:                -79.948
No. Observations:                  32   AIC:                             167.9
Df Residuals:                      28   BIC:                             173.8
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
===============================================================================
                  coef    std err          t      P&gt;|t|      [0.025      0.975]
-------------------------------------------------------------------------------
Intercept      28.6501      1.588     18.044      0.000      25.398      31.903
C(cyl)[T.6]    -5.9677      1.639     -3.640      0.001      -9.326      -2.610
C(cyl)[T.8]    -8.5209      2.326     -3.663      0.001     -13.286      -3.756
hp             -0.0240      0.015     -1.560      0.130      -0.056       0.008
==============================================================================
Omnibus:                        0.251   Durbin-Watson:                   1.667
Prob(Omnibus):                  0.882   Jarque-Bera (JB):                0.417
Skew:                           0.163   Prob(JB):                        0.812
Kurtosis:                       2.545   Cond. No.                         766.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>We can see here that two of the three possible values of <code class="docutils literal notranslate"><span class="pre">cyl</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mpg</span><span class="p">[</span><span class="s1">&#39;cyl&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;6&#39;, &#39;4&#39;, &#39;8&#39;]
Categories (3, object): [&#39;6&#39;, &#39;4&#39;, &#39;8&#39;]
</pre></div>
</div>
</div>
</div>
<p>have been added as fixed effects regressors. The way that <code class="docutils literal notranslate"><span class="pre">+C(cyl)</span></code> has been added makes it so that the coefficients given are relative to the coefficient for the intercept. We can turn the intercept off to get a coefficient per unique <code class="docutils literal notranslate"><span class="pre">cyl</span></code> value:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mpg ~ hp + C(cyl) -1&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span>
         <span class="o">.</span><span class="n">fit</span><span class="p">()</span>
         <span class="o">.</span><span class="n">summary</span><span class="p">()</span>
         <span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
C(cyl)[4]     28.6501      1.588     18.044      0.000      25.398      31.903
C(cyl)[6]     22.6825      2.228     10.180      0.000      18.119      27.246
C(cyl)[8]     20.1293      3.331      6.042      0.000      13.305      26.953
hp            -0.0240      0.015     -1.560      0.130      -0.056       0.008
==============================================================================
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="transformations-of-regressors">
<h2><span class="section-number">3.4. </span>Transformations of regressors<a class="headerlink" href="#transformations-of-regressors" title="Permalink to this headline">¶</a></h2>
<p>This chapter is showcasing <em>linear</em> regression. What that means is that the model is linear in the regressors: but it doesn’t mean that those regressors can’t be some kind of (potentially non-linear) transform of the original features <span class="math notranslate nohighlight">\(x_i\)</span>.</p>
<div class="section" id="logs-and-arcsinh">
<h3><span class="section-number">3.4.1. </span>Logs and arcsinh<a class="headerlink" href="#logs-and-arcsinh" title="Permalink to this headline">¶</a></h3>
<p>You have two options for adding in logs: do them before, or do them in the formula. Doing them before just makes use of standard dataframe operations to declare a new column:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mpg</span><span class="p">[</span><span class="s1">&#39;lnhp&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mpg</span><span class="p">[</span><span class="s1">&#39;hp&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mpg ~ lnhp&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span>
         <span class="o">.</span><span class="n">fit</span><span class="p">()</span>
         <span class="o">.</span><span class="n">summary</span><span class="p">()</span>
         <span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     72.6405      6.004     12.098      0.000      60.378      84.903
lnhp         -10.7642      1.224     -8.792      0.000     -13.265      -8.264
==============================================================================
</pre></div>
</div>
</div>
</div>
<p>Alternatively, you can specify the log directly in the formula:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_ln</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mpg ~ np.log(hp)&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_ln</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     72.6405      6.004     12.098      0.000      60.378      84.903
np.log(hp)   -10.7642      1.224     -8.792      0.000     -13.265      -8.264
==============================================================================
</pre></div>
</div>
</div>
</div>
<p>Clearly, the first method will work for <code class="docutils literal notranslate"><span class="pre">arcsinh(x)</span></code> and <code class="docutils literal notranslate"><span class="pre">log(x+1)</span></code>, but you can also pass both of these into the formula directly too. (For more on the pros and cons of arcsinh, see <span id="id1">[<a class="reference internal" href="zreferences.html#id6"><span>7</span></a>]</span>.) Here it is with arcsinh:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mpg ~ np.arcsinh(hp)&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span>
         <span class="o">.</span><span class="n">fit</span><span class="p">()</span>
         <span class="o">.</span><span class="n">summary</span><span class="p">()</span>
         <span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==================================================================================
                     coef    std err          t      P&gt;|t|      [0.025      0.975]
----------------------------------------------------------------------------------
Intercept         80.1041      6.850     11.694      0.000      66.115      94.094
np.arcsinh(hp)   -10.7646      1.224     -8.792      0.000     -13.265      -8.264
==================================================================================
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="interaction-terms-and-powers">
<h3><span class="section-number">3.4.2. </span>Interaction terms and powers<a class="headerlink" href="#interaction-terms-and-powers" title="Permalink to this headline">¶</a></h3>
<p>This chapter is showcasing <em>linear</em> regression. What that means is that the model is linear in the regressors: but it doesn’t mean that those regressors can’t be some kind of non-linear transform of the original features <span class="math notranslate nohighlight">\(x_i\)</span>. Two of the most common transformations that you might want to use are <em>interaction terms</em> and <em>polynomial terms</em>. An example of an interaction term would be</p>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1 x_1 \cdot x_2
\]</div>
<p>while an example of a polynomial term would be</p>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1 x_1^2
\]</div>
<p>i.e. the last term enters only after it is multiplied by itself.</p>
<p>One note of warning: the interpretation of the effect of a variable is no longer as simple as was set out at the start of this chapter. To work out <em>what</em> the new interpretation is, the procedure is the same though: just take the derivative. In the case of the interaction model above, the effect of a unit change in <span class="math notranslate nohighlight">\(x_1\)</span> on <span class="math notranslate nohighlight">\(y\)</span> is now going to be a function of <span class="math notranslate nohighlight">\(x_2\)</span>. In the case of the polynomial model above, the effect of a unit change in <span class="math notranslate nohighlight">\(x_1\)</span> on <span class="math notranslate nohighlight">\(y\)</span> will be <span class="math notranslate nohighlight">\(2\beta_1 \cdot x_1\)</span>. For more on interaction terms, see <span id="id2">[<a class="reference internal" href="zreferences.html#id7"><span>8</span></a>]</span>.</p>
<p>Alright, with all of that preamble out of the way, let’s see how we actual do some of this! Let’s try including a linear and squared term in the regression of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> on <code class="docutils literal notranslate"><span class="pre">hp</span></code> making use of the numpy power function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">res_poly</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mpg ~ hp + np.power(hp, 2)&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res_poly</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>===================================================================================
                      coef    std err          t      P&gt;|t|      [0.025      0.975]
-----------------------------------------------------------------------------------
Intercept          40.4091      2.741     14.744      0.000      34.804      46.015
hp                 -0.2133      0.035     -6.115      0.000      -0.285      -0.142
np.power(hp, 2)     0.0004   9.84e-05      4.275      0.000       0.000       0.001
===================================================================================
</pre></div>
</div>
</div>
</div>
<p>Now let’s include the original term in hp, a term in disp, and the interaction between them, which is represented by hp:disp in the table.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">res_inter</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mpg ~ hp * disp&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res_inter</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     39.6743      2.914     13.614      0.000      33.705      45.644
hp            -0.0979      0.025     -3.956      0.000      -0.149      -0.047
disp          -0.0734      0.014     -5.100      0.000      -0.103      -0.044
hp:disp        0.0003   8.69e-05      3.336      0.002       0.000       0.000
==============================================================================
</pre></div>
</div>
</div>
</div>
<p>In the unusual case that you want <em>only</em> the interaction term, you write it as it appears in the table above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mpg ~ hp : disp&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     25.8099      1.005     25.677      0.000      23.757      27.863
hp:disp       -0.0001   1.91e-05     -7.409      0.000      -0.000      -0.000
==============================================================================
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="the-formula-api-explained">
<h2><span class="section-number">3.5. </span>The formula API explained<a class="headerlink" href="#the-formula-api-explained" title="Permalink to this headline">¶</a></h2>
<p>As you will have seen <code class="docutils literal notranslate"><span class="pre">~</span></code> separates the left- and right-hand sides of the regression. <code class="docutils literal notranslate"><span class="pre">+</span></code> computes a set union, which will also be familiar from the examples above (ie it inludes two terms as long as they are distinct). <code class="docutils literal notranslate"><span class="pre">-</span></code> computes a set difference; it adds the set of terms to the left of it while removing any that appear on the right of it. As we’ve seen, <code class="docutils literal notranslate"><span class="pre">a*b</span></code> is a short-hand for <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">+</span> <span class="pre">b</span> <span class="pre">+</span> <span class="pre">a:b</span></code>, with the last term representing the interaction. <code class="docutils literal notranslate"><span class="pre">/</span></code> is short hand for <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">+</span> <span class="pre">a:b</span></code>, which is useful if, for example <code class="docutils literal notranslate"><span class="pre">b</span></code> is nested within <code class="docutils literal notranslate"><span class="pre">a</span></code>, so it doesn’t make sense to control for <code class="docutils literal notranslate"><span class="pre">b</span></code> on its own. Actually, the <code class="docutils literal notranslate"><span class="pre">:</span></code> character can interact multiple terms so that <code class="docutils literal notranslate"><span class="pre">(a</span> <span class="pre">+</span> <span class="pre">b):(d</span> <span class="pre">+</span> <span class="pre">c)</span></code> is the same as <code class="docutils literal notranslate"><span class="pre">a:c</span> <span class="pre">+</span> <span class="pre">a:d</span> <span class="pre">+</span> <span class="pre">b:c</span> <span class="pre">+</span> <span class="pre">b:d</span></code>. <code class="docutils literal notranslate"><span class="pre">C(a)</span></code> tells statsmodels to treat <code class="docutils literal notranslate"><span class="pre">a</span></code> as a categorical variable that will be included as a fixed effect. Finally, as we saw above with powers, you can also pass in vectorised functions, such as <code class="docutils literal notranslate"><span class="pre">np.log</span></code> and <code class="docutils literal notranslate"><span class="pre">np.power</span></code>, directly into the formulae.</p>
<p>One gotcha with the formula API is ensuring that you have sensible variable names in your dataframe, i.e. ones that do <em>not</em> include whitespace or, to take a really pathological example, have the name ‘a + b’ for one of the columns that you want to regress on. You can dodge this kind of problem by passing in the variable name as, for example, <code class="docutils literal notranslate"><span class="pre">Q(&quot;a</span> <span class="pre">+</span> <span class="pre">b&quot;)</span></code> to be clear that the <em>column name</em> is anything within the <code class="docutils literal notranslate"><span class="pre">Q(&quot;...&quot;)</span></code>.</p>
</div>
<div class="section" id="multiple-regression-models">
<h2><span class="section-number">3.6. </span>Multiple regression models<a class="headerlink" href="#multiple-regression-models" title="Permalink to this headline">¶</a></h2>
<p>As is so often the case, you’re likely to want to run more than one model at once with different specifications. Although there is a base version of this in <strong>statsmodels</strong>, called <code class="docutils literal notranslate"><span class="pre">summary_col</span></code>, which you can find an example of <a class="reference external" href="http://aeturrell.com//2018/05/05/running-many-regressions-alongside-pandas/">here</a>, instead we’ll be using the <a class="reference external" href="https://github.com/mwburke/stargazer"><strong>stargazer</strong></a> package to assemble the regressions together in a table.</p>
<p>In the above examples, we’ve collected a few different regression results. Let’s put them together:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">stargazer.stargazer</span> <span class="kn">import</span> <span class="n">Stargazer</span>


<span class="n">stargazer_tab</span> <span class="o">=</span> <span class="n">Stargazer</span><span class="p">([</span><span class="n">results_ln</span><span class="p">,</span> <span class="n">res_poly</span><span class="p">,</span> <span class="n">res_inter</span><span class="p">])</span>
<span class="n">stargazer_tab</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table style="text-align:center"><tr><td colspan="4" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"></td><td colspan="3"><em>Dependent variable:mpg</em></td></tr><tr><td style="text-align:left"></td><tr><td style="text-align:left"></td><td>(1)</td><td>(2)</td><td>(3)</td></tr><tr><td colspan="4" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">Intercept</td><td>72.640<sup>***</sup></td><td>40.409<sup>***</sup></td><td>39.674<sup>***</sup></td></tr><tr><td style="text-align:left"></td><td>(6.004)</td><td>(2.741)</td><td>(2.914)</td></tr><tr><td style="text-align:left">disp</td><td></td><td></td><td>-0.073<sup>***</sup></td></tr><tr><td style="text-align:left"></td><td></td><td></td><td>(0.014)</td></tr><tr><td style="text-align:left">hp</td><td></td><td>-0.213<sup>***</sup></td><td>-0.098<sup>***</sup></td></tr><tr><td style="text-align:left"></td><td></td><td>(0.035)</td><td>(0.025)</td></tr><tr><td style="text-align:left">hp:disp</td><td></td><td></td><td>0.000<sup>***</sup></td></tr><tr><td style="text-align:left"></td><td></td><td></td><td>(0.000)</td></tr><tr><td style="text-align:left">np.log(hp)</td><td>-10.764<sup>***</sup></td><td></td><td></td></tr><tr><td style="text-align:left"></td><td>(1.224)</td><td></td><td></td></tr><tr><td style="text-align:left">np.power(hp, 2)</td><td></td><td>0.000<sup>***</sup></td><td></td></tr><tr><td style="text-align:left"></td><td></td><td>(0.000)</td><td></td></tr><td colspan="4" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align: left">Observations</td><td>32</td><td>32</td><td>32</td></tr><tr><td style="text-align: left">R<sup>2</sup></td><td>0.720</td><td>0.756</td><td>0.820</td></tr><tr><td style="text-align: left">Adjusted R<sup>2</sup></td><td>0.711</td><td>0.739</td><td>0.801</td></tr><tr><td style="text-align: left">Residual Std. Error</td><td>3.239 (df=30)</td><td>3.077 (df=29)</td><td>2.692 (df=28)</td></tr><tr><td style="text-align: left">F Statistic</td><td>77.301<sup>***</sup> (df=1; 30)</td><td>44.953<sup>***</sup> (df=2; 29)</td><td>42.475<sup>***</sup> (df=3; 28)</td></tr><tr><td colspan="4" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align: left">Note:</td>
 <td colspan="3" style="text-align: right">
  <sup>*</sup>p&lt;0.1;
  <sup>**</sup>p&lt;0.05;
  <sup>***</sup>p&lt;0.01
 </td></tr></table></div></div>
</div>
<p>There are lots of customisation options, including ones that add a title, rename variables, add notes, and so on. What is most useful is that as well as the HTML friendly output that you can see above, the package also exports to latex:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">stargazer_tab</span><span class="o">.</span><span class="n">render_latex</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>\begin{table}[!htbp] \centering
\begin{tabular}{@{\extracolsep{5pt}}lccc}
\\[-1.8ex]\hline
\hline \\[-1.8ex]
&amp; \multicolumn{3}{c}{\textit{Dependent variable:}} \
\cr \cline{3-4}
\\[-1.8ex] &amp; (1) &amp; (2) &amp; (3) \\
\hline \\[-1.8ex]
 Intercept &amp; 72.640$^{***}$ &amp; 40.409$^{***}$ &amp; 39.674$^{***}$ \\
  &amp; (6.004) &amp; (2.741) &amp; (2.914) \\
 disp &amp; &amp; &amp; -0.073$^{***}$ \\
  &amp; &amp; &amp; (0.014) \\
 hp &amp; &amp; -0.213$^{***}$ &amp; -0.098$^{***}$ \\
  &amp; &amp; (0.035) &amp; (0.025) \\
 hp:disp &amp; &amp; &amp; 0.000$^{***}$ \\
  &amp; &amp; &amp; (0.000) \\
 np.log(hp) &amp; -10.764$^{***}$ &amp; &amp; \\
  &amp; (1.224) &amp; &amp; \\
 np.power(hp, 2) &amp; &amp; 0.000$^{***}$ &amp; \\
  &amp; &amp; (0.000) &amp; \\
\hline \\[-1.8ex]
 Observations &amp; 32 &amp; 32 &amp; 32 \\
 $R^2$ &amp; 0.720 &amp; 0.756 &amp; 0.820 \\
 Adjusted $R^2$ &amp; 0.711 &amp; 0.739 &amp; 0.801 \\
 Residual Std. Error &amp; 3.239(df = 30) &amp; 3.077(df = 29) &amp; 2.692(df = 28)  \\
 F Statistic &amp; 77.301$^{***}$ (df = 1.0; 30.0) &amp; 44.953$^{***}$ (df = 2.0; 29.0) &amp; 42.475$^{***}$ (df = 3.0; 28.0) \\
\hline
\hline \\[-1.8ex]
\textit{Note:} &amp; \multicolumn{3}{r}{$^{*}$p$&lt;$0.1; $^{**}$p$&lt;$0.05; $^{***}$p$&lt;$0.01} \\
\end{tabular}
\end{table}
</pre></div>
</div>
</div>
</div>
<p>And of course this can be written to a file using <code class="docutils literal notranslate"><span class="pre">open('regression.tex',</span> <span class="pre">'w').write(stargazer.render_latex())</span></code> where you can get your main latex compilation to scoop it up and use it.</p>
</div>
<div class="section" id="specifying-regressions-without-formulae-using-the-array-api">
<h2><span class="section-number">3.7. </span>Specifying regressions without formulae, using the array API<a class="headerlink" href="#specifying-regressions-without-formulae-using-the-array-api" title="Permalink to this headline">¶</a></h2>
<p>As noted, there are two ways to run regressions in <a class="reference external" href="https://www.statsmodels.org/stable/index.html"><strong>statsmodels</strong></a>; passing the data directly as objects, and using formulae. We’ve seen the formula API, now let’s see how to specify regressions using arrays with the format <code class="docutils literal notranslate"><span class="pre">sm.OLS(y,</span> <span class="pre">X)</span></code>.</p>
<p>We will first need to take the data out of the <strong>pandas</strong> dataframe and put it into a couple of arrays. When we’re not using the formula API, the default is to treat the array X as the design matrix for the regression-so, if it doesn’t have a column of constants in, there will be no intercept in the regression. Therefore, we need to add a constant vector to the matrix <code class="docutils literal notranslate"><span class="pre">X</span></code> if we <em>do</em> want an intercept. Use <code class="docutils literal notranslate"><span class="pre">sm.add_constant(X)</span></code> for this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">xf</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">xf</span><span class="p">[</span><span class="s1">&#39;mass&#39;</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.014
Model:                            OLS   Adj. R-squared:                 -0.005
Method:                 Least Squares   F-statistic:                    0.7446
Date:                Sun, 28 Feb 2021   Prob (F-statistic):              0.392
Time:                        13:26:58   Log-Likelihood:                -361.23
No. Observations:                  55   AIC:                             726.5
Df Residuals:                      53   BIC:                             730.5
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         -8.7955    127.191     -0.069      0.945    -263.909     246.318
x1             0.6159      0.714      0.863      0.392      -0.816       2.048
==============================================================================
Omnibus:                      121.086   Durbin-Watson:                   2.029
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5943.317
Skew:                           7.095   Prob(JB):                         0.00
Kurtosis:                      51.909   Cond. No.                         958.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>This approach seems a lot less convenient, not to mention less clear, so you may be wondering when it is useful. It’s useful when you want to do many regressions in a systematic way or when you don’t know what the columns of a dataset will be called ahead of time. It can actually be a little bit simpler to specify for more complex regressions too.</p>
<div class="section" id="fixed-effects-in-the-array-api">
<h3><span class="section-number">3.7.1. </span>Fixed effects in the array API<a class="headerlink" href="#fixed-effects-in-the-array-api" title="Permalink to this headline">¶</a></h3>
<p>If you’re using the formula API, it’s easy to turn a regressor <code class="docutils literal notranslate"><span class="pre">x</span></code> into a fixed effect by putting <code class="docutils literal notranslate"><span class="pre">C(x)</span></code> into the model formula, as you’ll see in the next section.</p>
<p>For the array API, things are not that simple and you need to use dummy variables. Let’s say we have some data like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">Generator</span><span class="p">,</span> <span class="n">PCG64</span>

<span class="c1"># Set seed for random numbers</span>
<span class="n">seed_for_prng</span> <span class="o">=</span> <span class="mi">78557</span>
<span class="n">prng</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">PCG64</span><span class="p">(</span><span class="n">seed_for_prng</span><span class="p">))</span>

<span class="n">no_obs</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">prng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">no_obs</span><span class="p">))</span>
<span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">prng</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">no_obs</span><span class="p">)</span>
<span class="c1"># Get this a numpy array</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">values</span>
<span class="c1"># Create the y data, adding in a bit of noise</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">prng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">no_obs</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">el_y</span> <span class="o">+</span> <span class="mf">1.5</span> <span class="k">if</span> <span class="n">el_x</span> <span class="o">==</span> <span class="s1">&#39;a&#39;</span> <span class="k">else</span> <span class="n">el_y</span> <span class="o">+</span> <span class="mf">3.4</span> <span class="k">for</span> <span class="n">el_y</span><span class="p">,</span> <span class="n">el_x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])]</span>
<span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.927388713624307, &#39;b&#39;],
       [-0.05975401040013389, &#39;a&#39;],
       [1.0437358922863453, &#39;a&#39;],
       [1.5056188793532617, &#39;b&#39;],
       [0.10762743224577262, &#39;a&#39;]], dtype=object)
</pre></div>
</div>
</div>
</div>
<p>The first feature (column) is of numbers and it’s clear how we include it. The second, however, is a grouping that we’d like to include as a fixed effect. But if we just throw this matrix into <code class="docutils literal notranslate"><span class="pre">sm.OLS(y,</span> <span class="pre">X)</span></code>, we’re going to get trouble because <strong>statsmodels</strong> isn’t sure what to do with a vector of strings. So, instead, we need to create some dummy variables out of our second column of data</p>
<p>Astonishingly, there are several popular ways to create dummy variables in Python: <strong>scikit-learn</strong>’s <code class="docutils literal notranslate"><span class="pre">OneHotEncoder</span></code> and <strong>pandas</strong>’ <code class="docutils literal notranslate"><span class="pre">get_dummies</span></code> being my favourites. Let’s use the latter here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>a</th>
      <th>b</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>197</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>198</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>199</th>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>200 rows × 2 columns</p>
</div></div></div>
</div>
<p>We just need to pop this into our matrix <span class="math notranslate nohighlight">\(X\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 0.92738871,  0.        ,  1.        ],
       [-0.05975401,  1.        ,  0.        ],
       [ 1.04373589,  1.        ,  0.        ],
       [ 1.50561888,  0.        ,  1.        ],
       [ 0.10762743,  1.        ,  0.        ]])
</pre></div>
</div>
</div>
</div>
<p>Okay, so now we’re ready to do our regression:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.998
Model:                            OLS   Adj. R-squared:                  0.998
Method:                 Least Squares   F-statistic:                 4.302e+04
Date:                Sun, 28 Feb 2021   Prob (F-statistic):          6.90e-261
Time:                        13:26:58   Log-Likelihood:                 165.45
No. Observations:                 200   AIC:                            -324.9
Df Residuals:                     197   BIC:                            -315.0
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
x1             1.9893      0.008    263.045      0.000       1.974       2.004
x2             1.9924      0.011    176.203      0.000       1.970       2.015
x3             3.8977      0.010    384.415      0.000       3.878       3.918
==============================================================================
Omnibus:                        2.096   Durbin-Watson:                   2.116
Prob(Omnibus):                  0.351   Jarque-Bera (JB):                2.017
Skew:                           0.245   Prob(JB):                        0.365
Kurtosis:                       2.955   Cond. No.                         1.50
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>Perhaps you can see why I generally prefer the formula API…</p>
</div>
</div>
<div class="section" id="instrumental-variables">
<h2><span class="section-number">3.8. </span>Instrumental variables<a class="headerlink" href="#instrumental-variables" title="Permalink to this headline">¶</a></h2>
<p>Rather than use <strong>statsmodels</strong> for IV, we’ll use the <a class="reference external" href="https://bashtage.github.io/linearmodels/doc/index.html"><strong>linearmodels</strong></a> package, which has very clean documentation (indeed, this sub-section is indebted to that documentation).</p>
<p>Recall that in IV regression, we have a model of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}y_i    &amp; = x_{1i}\hat{\beta_1} + x_{2i}\hat{\beta_2} + \epsilon_i \\
x_{2i} &amp; = z_{1i}\hat{\delta} + z_{2i}\hat{\gamma} + \nu_i\end{split}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{1i}\)</span> is a set of <span class="math notranslate nohighlight">\(k_1\)</span> exogenous regressors and <span class="math notranslate nohighlight">\(x_{2i}\)</span> is a set of <span class="math notranslate nohighlight">\(k_2\)</span> endogenous regressors such that <span class="math notranslate nohighlight">\(\text{Cov}(x_{2i}, \epsilon_i)\neq 0\)</span>. This is a problem for the usual OLS assumptions (the right-hand side should be exogenous).</p>
<p>To get around this, in 2-stage least squares IV, we first regress <span class="math notranslate nohighlight">\(x_{2i}\)</span> on instruments that explain <span class="math notranslate nohighlight">\(x_{2i}\)</span> <em>but not</em> <span class="math notranslate nohighlight">\(y_i\)</span>, and then regress <span class="math notranslate nohighlight">\(y_i\)</span> only on the predicted/estimated left-hand side from the first regression, ie on <span class="math notranslate nohighlight">\(\hat{x_{2i}}\)</span>. There are other estimators than IV2SLS, but I think that one has the most intuitive explanation of what’s going.</p>
<p>As well as a 2-stage least squares estimator called <code class="docutils literal notranslate"><span class="pre">IV2SLS</span></code>, <strong>linearmodels</strong> has a Limited Information Maximum Likelihood (LIML) estimator <code class="docutils literal notranslate"><span class="pre">IVLIML</span></code>, a Generalized Method of Moments (GMM) estimator <code class="docutils literal notranslate"><span class="pre">IVGMM</span></code>, and a Generalized Method of Moments using the Continuously Updating Estimator (CUE) <code class="docutils literal notranslate"><span class="pre">IVGMMCUE</span></code>.</p>
<p>Just as with OLS via <strong>statsmodels</strong>, there’s an option to use an array API for the <strong>linearmodels</strong> IV methods.</p>
<p>It’s always easiest to see an example, so let’s estimate what might cause (realised) cigarette demand for the 48 continental US states in 1995 with <code class="docutils literal notranslate"><span class="pre">IV2SLS</span></code>. First we need to import the estimator, <code class="docutils literal notranslate"><span class="pre">IV2SLS</span></code>, and the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">linearmodels.iv</span> <span class="kn">import</span> <span class="n">IV2SLS</span>

<span class="n">df</span> <span class="o">=</span> <span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://vincentarelbundock.github.io/Rdatasets/csv/AER/CigarettesSW.csv&#39;</span><span class="p">,</span>
                   <span class="n">dtype</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;state&#39;</span><span class="p">:</span> <span class="s1">&#39;category&#39;</span><span class="p">,</span> <span class="s1">&#39;year&#39;</span><span class="p">:</span> <span class="s1">&#39;category&#39;</span><span class="p">})</span>
        <span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">rprice</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;cpi&#39;</span><span class="p">],</span>
                <span class="n">rincome</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;income&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;population&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;cpi&#39;</span><span class="p">])</span>
     <span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>state</th>
      <th>year</th>
      <th>cpi</th>
      <th>population</th>
      <th>packs</th>
      <th>income</th>
      <th>tax</th>
      <th>price</th>
      <th>taxs</th>
      <th>rprice</th>
      <th>rincome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>AL</td>
      <td>1985</td>
      <td>1.076</td>
      <td>3973000.0</td>
      <td>116.486282</td>
      <td>46014968</td>
      <td>32.500004</td>
      <td>102.181671</td>
      <td>33.348335</td>
      <td>94.964381</td>
      <td>10.763866</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>AR</td>
      <td>1985</td>
      <td>1.076</td>
      <td>2327000.0</td>
      <td>128.534592</td>
      <td>26210736</td>
      <td>37.000000</td>
      <td>101.474998</td>
      <td>37.000000</td>
      <td>94.307622</td>
      <td>10.468165</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>AZ</td>
      <td>1985</td>
      <td>1.076</td>
      <td>3184000.0</td>
      <td>104.522614</td>
      <td>43956936</td>
      <td>31.000000</td>
      <td>108.578751</td>
      <td>36.170418</td>
      <td>100.909622</td>
      <td>12.830456</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>CA</td>
      <td>1985</td>
      <td>1.076</td>
      <td>26444000.0</td>
      <td>100.363037</td>
      <td>447102816</td>
      <td>26.000000</td>
      <td>107.837341</td>
      <td>32.104000</td>
      <td>100.220580</td>
      <td>15.713321</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>CO</td>
      <td>1985</td>
      <td>1.076</td>
      <td>3209000.0</td>
      <td>112.963539</td>
      <td>49466672</td>
      <td>31.000000</td>
      <td>94.266663</td>
      <td>31.000000</td>
      <td>87.608425</td>
      <td>14.326190</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now we’ll specify the model. It’s going to be in the form <code class="docutils literal notranslate"><span class="pre">dep</span> <span class="pre">~</span> <span class="pre">exog</span> <span class="pre">+</span> <span class="pre">[endog</span> <span class="pre">~</span> <span class="pre">instruments]</span></code>, where endog will be regressed on instruments and dep will be regressed on both exog and the predicted values of endog.</p>
<p>In this case, the model will be</p>
<div class="math notranslate nohighlight">
\[
\text{Price}_i = \hat{\pi_0} + \hat{\pi_1} \text{SalesTax}_i + v_i 
\]</div>
<p>in the first stage regression and</p>
<div class="math notranslate nohighlight">
\[
\text{Packs}_i = \hat{\beta_0} + \hat{\beta_2}\widehat{\text{Price}_i} + \hat{\beta_1} \text{RealIncome}_i + u_i
\]</div>
<p>in the second stage.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_iv2sls</span> <span class="o">=</span> <span class="p">(</span><span class="n">IV2SLS</span><span class="o">.</span><span class="n">from_formula</span><span class="p">(</span><span class="s1">&#39;np.log(packs) ~ 1 + np.log(rincome) + C(year) + C(state) + [np.log(rprice) ~ taxs]&#39;</span><span class="p">,</span>
                                      <span class="n">df</span><span class="p">)</span>
                        <span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span><span class="o">=</span><span class="s1">&#39;clustered&#39;</span><span class="p">,</span> <span class="n">clusters</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_iv2sls</span><span class="o">.</span><span class="n">summary</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                          IV-2SLS Estimation Summary                          
==============================================================================
Dep. Variable:          np.log(packs)   R-squared:                      0.9659
Estimator:                    IV-2SLS   Adj. R-squared:                 0.9279
No. Observations:                  96   F-statistic:                  2.66e+16
Date:                Sun, Feb 28 2021   P-value (F-stat)                0.0000
Time:                        13:26:59   Distribution:                 chi2(50)
Cov. Estimator:             clustered                                         
                                                                              
                                Parameter Estimates                                
===================================================================================
                 Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI
-----------------------------------------------------------------------------------
Intercept           9.4924     0.0263     360.24     0.0000      9.4407      9.5440
C(year)[T.1995]    -0.0328                                                         
C(state)[T.AR]      0.1770     0.0531     3.3338     0.0009      0.0729      0.2810
C(state)[T.AZ]     -0.0899     0.0132    -6.8132     0.0000     -0.1158     -0.0640
C(state)[T.CA]     -0.2781     0.0214    -12.996     0.0000     -0.3200     -0.2361
C(state)[T.CO]     -0.2479     0.0090    -27.625     0.0000     -0.2655     -0.2303
C(state)[T.CT]     -0.0171     0.0196    -0.8720     0.3832     -0.0556      0.0213
C(state)[T.DE]      0.1110     0.0291     3.8105     0.0001      0.0539      0.1682
C(state)[T.FL]      0.0762     0.0142     5.3596     0.0000      0.0483      0.1041
C(state)[T.GA]     -0.0695     0.0251    -2.7706     0.0056     -0.1186     -0.0203
C(state)[T.IA]      0.0120     0.0739     0.1629     0.8706     -0.1328      0.1569
C(state)[T.ID]     -0.1272     0.0077    -16.597     0.0000     -0.1423     -0.1122
C(state)[T.IL]     -0.0339     0.0081    -4.1912     0.0000     -0.0497     -0.0180
C(state)[T.IN]      0.1198     0.0611     1.9609     0.0499   5.573e-05      0.2395
C(state)[T.KS]     -0.0910     0.0305    -2.9884     0.0028     -0.1507     -0.0313
C(state)[T.KY]      0.3525     0.0631     5.5906     0.0000      0.2289      0.4761
C(state)[T.LA]      0.1315     0.0104     12.664     0.0000      0.1112      0.1519
C(state)[T.MA]     -0.0403     0.0069    -5.8826     0.0000     -0.0538     -0.0269
C(state)[T.MD]     -0.2322     0.0239    -9.7376     0.0000     -0.2790     -0.1855
C(state)[T.ME]      0.2008     0.0574     3.5011     0.0005      0.0884      0.3133
C(state)[T.MI]      0.1268     0.0745     1.7009     0.0890     -0.0193      0.2728
C(state)[T.MN]      0.0568     0.0490     1.1595     0.2463     -0.0392      0.1529
C(state)[T.MO]      0.0640     0.0476     1.3454     0.1785     -0.0292      0.1572
C(state)[T.MS]      0.1501     0.0272     5.5267     0.0000      0.0969      0.2034
C(state)[T.MT]     -0.1522     0.0054    -28.250     0.0000     -0.1627     -0.1416
C(state)[T.NC]      0.0396     0.0191     2.0655     0.0389      0.0020      0.0771
C(state)[T.ND]     -0.0311     0.0399    -0.7787     0.4361     -0.1092      0.0471
C(state)[T.NE]     -0.0741     0.0375    -1.9765     0.0481     -0.1476     -0.0006
C(state)[T.NH]      0.3504     0.0315     11.114     0.0000      0.2886      0.4122
C(state)[T.NJ]     -0.0873  6.107e-05    -1429.3     0.0000     -0.0874     -0.0872
C(state)[T.NM]     -0.2858     0.0040    -71.049     0.0000     -0.2937     -0.2779
C(state)[T.NV]      0.1789     0.0259     6.9075     0.0000      0.1281      0.2296
C(state)[T.NY]     -0.0719     0.0032    -22.256     0.0000     -0.0782     -0.0655
C(state)[T.OH]      0.0325     0.0402     0.8088     0.4186     -0.0463      0.1114
C(state)[T.OK]      0.0946     0.0538     1.7572     0.0789     -0.0109      0.2000
C(state)[T.OR]     -0.0153     0.0673    -0.2269     0.8205     -0.1471      0.1166
C(state)[T.PA]     -0.0031     0.0006    -4.8401     0.0000     -0.0044     -0.0019
C(state)[T.RI]      0.1394     0.0921     1.5136     0.1301     -0.0411      0.3200
C(state)[T.SC]     -0.0212     0.0334    -0.6345     0.5257     -0.0866      0.0442
C(state)[T.SD]     -0.0675     0.0711    -0.9488     0.3427     -0.2069      0.0719
C(state)[T.TN]      0.1473     0.0470     3.1340     0.0017      0.0552      0.2394
C(state)[T.TX]     -0.0579     0.0136    -4.2560     0.0000     -0.0845     -0.0312
C(state)[T.UT]     -0.4899     0.0276    -17.776     0.0000     -0.5440     -0.4359
C(state)[T.VA]     -0.0559     0.0471    -1.1875     0.2350     -0.1482      0.0364
C(state)[T.VT]      0.2209     0.0467     4.7267     0.0000      0.1293      0.3125
C(state)[T.WA]      0.0064     0.0011     6.0151     0.0000      0.0043      0.0085
C(state)[T.WI]      0.0741     0.0590     1.2569     0.2088     -0.0415      0.1897
C(state)[T.WV]      0.1576     0.0582     2.7097     0.0067      0.0436      0.2716
C(state)[T.WY]     -0.0169     0.0590    -0.2858     0.7750     -0.1325      0.0988
np.log(rincome)     0.4434  6.274e-08  7.068e+06     0.0000      0.4434      0.4434
np.log(rprice)     -1.2793  8.704e-09  -1.47e+08     0.0000     -1.2793     -1.2793
===================================================================================

Endogenous: np.log(rprice)
Instruments: taxs
Clustered Covariance (One-Way)
Debiased: False
Num Clusters: 2
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/arthurturrell/opt/anaconda3/envs/codeforecon/lib/python3.8/site-packages/linearmodels/iv/results.py:189: RuntimeWarning: invalid value encountered in sqrt
  std_errors = sqrt(diag(self.cov))
</pre></div>
</div>
</div>
</div>
<p>We sort of skipped a step here and did everything all in one go. If we <em>did</em> want to know how our first stage regression went, we can just pass a formula to <code class="docutils literal notranslate"><span class="pre">IV2SLS</span></code> without the part in square brackets, <code class="docutils literal notranslate"><span class="pre">[...]</span></code>, and it will run regular OLS.</p>
<p>But, in this case, there’s an easier way: we can print out a set of handy 1st stage statistics from running the full model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">results_iv2sls</span><span class="o">.</span><span class="n">first_stage</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      First Stage Estimation Results     
=========================================
                           np.log(rprice)
-----------------------------------------
R-squared                          0.9760
Partial R-squared                  0.7070
Shea&#39;s R-squared                   0.7070
Partial F-statistic             4.104e+28
P-value (Partial F-stat)           0.0000
Partial F-stat Distn              chi2(1)
==========================   ============
Intercept                          4.4375
                                 (410.34)
C(year)[T.1995]                    0.0822
                               (        )
C(state)[T.AR]                    -0.0190
                                (-3.5717)
C(state)[T.AZ]                     0.0552
                                 (20.534)
C(state)[T.CA]                     0.0969
                                 (5.3807)
C(state)[T.CO]                    -0.0044
                                (-0.1117)
C(state)[T.CT]                     0.1269
                                 (9.0641)
C(state)[T.DE]                     0.0254
                                 (7.9851)
C(state)[T.FL]                     0.0598
                                 (4.0067)
C(state)[T.GA]                    -0.0049
                                (-0.3455)
C(state)[T.IA]                     0.0135
                                 (0.5305)
C(state)[T.ID]                     0.0297
                                 (79.832)
C(state)[T.IL]                     0.0476
                                 (12.008)
C(state)[T.IN]                    -0.0460
                                (-76.127)
C(state)[T.KS]                    -0.0019
                                (-0.0855)
C(state)[T.KY]                    -0.0783
                                (-3.6663)
C(state)[T.LA]                     0.0298
                                 (2.6211)
C(state)[T.MA]                     0.0816
                                 (5.3671)
C(state)[T.MD]                    -0.0199
                                (-0.5429)
C(state)[T.ME]                     0.0370
                                 (1.8744)
C(state)[T.MI]                     0.0363
                                 (1.9899)
C(state)[T.MN]                     0.0932
                                 (8.6047)
C(state)[T.MO]                    -0.0056
                                (-12.033)
C(state)[T.MS]                     0.0147
                                 (2.1430)
C(state)[T.MT]                    -0.0183
                                (-9.6530)
C(state)[T.NC]                    -0.0672
                                (-1.8041)
C(state)[T.ND]                     0.0130
                                 (1.8765)
C(state)[T.NE]                     0.0105
                                 (2.4846)
C(state)[T.NH]                    -0.0175
                                (-0.6334)
C(state)[T.NJ]                     0.0694
                                 (5.2527)
C(state)[T.NM]                     0.0301
                                 (2.6464)
C(state)[T.NV]                     0.1075
                                 (16.434)
C(state)[T.NY]                     0.0847
                                 (5.1334)
C(state)[T.OH]                    -0.0197
                                (-22.592)
C(state)[T.OK]                    -0.0035
                                (-0.7109)
C(state)[T.OR]                     0.0146
                                 (0.2944)
C(state)[T.PA]                     0.0152
                                 (0.9986)
C(state)[T.RI]                     0.0249
                                 (0.5964)
C(state)[T.SC]                    -0.0530
                                (-2.1043)
C(state)[T.SD]                    -0.0190
                                (-1.4001)
C(state)[T.TN]                     0.0051
                                 (0.5517)
C(state)[T.TX]                     0.0378
                                 (4.0760)
C(state)[T.UT]                     0.0577
                                 (5.8788)
C(state)[T.VA]                     0.0265
                                 (0.5400)
C(state)[T.VT]                     0.0228
                                 (1.0969)
C(state)[T.WA]                     0.1554
                                 (13.780)
C(state)[T.WI]                     0.0723
                                 (5.1954)
C(state)[T.WV]                     0.0205
                                 (1.7212)
C(state)[T.WY]                    -0.0005
                                (-0.0220)
np.log(rincome)                   -0.0296
                               (        )
taxs                               0.0051
                               (2.94e+08)
-----------------------------------------

T-stats reported in parentheses
T-stats use same covariance type as original model
</pre></div>
</div>
</div>
</div>
<p>There are more tests and checks available. For example, Wooldridge’s regression test of exogeneity uses regression residuals from the endogenous variables regressed on the exogenous variables and the instrument to test for endogenity and is available to run on fitted model results. Let’s check that:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_iv2sls</span><span class="o">.</span><span class="n">wooldridge_regression</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Wooldridge&#39;s regression test of exogeneity
H0: Endogenous variables are exogenous
Statistic: 14000039221717696.0000
P-value: 0.0000
Distributed: chi2(1)
WaldTestStatistic, id: 0x7fd4a0bdc970
</pre></div>
</div>
</div>
</div>
<p>We can compare the IV results against (naive) OLS. First, run the OLS equivalent:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">res_cig_ols</span> <span class="o">=</span> <span class="p">(</span><span class="n">IV2SLS</span><span class="o">.</span><span class="n">from_formula</span><span class="p">(</span><span class="s1">&#39;np.log(packs) ~ 1 + np.log(rincome) + C(year) + C(state) + np.log(rprice)&#39;</span><span class="p">,</span>
                                      <span class="n">df</span><span class="p">)</span>
                        <span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span><span class="o">=</span><span class="s1">&#39;clustered&#39;</span><span class="p">,</span> <span class="n">clusters</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<p>Now select these two models to compare:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">linearmodels.iv.results</span> <span class="kn">import</span> <span class="n">compare</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
<span class="n">res</span><span class="p">[</span><span class="s1">&#39;OLS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">res_cig_ols</span>
<span class="n">res</span><span class="p">[</span><span class="s1">&#39;2SLS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">results_iv2sls</span>

<span class="nb">print</span><span class="p">(</span><span class="n">compare</span><span class="p">(</span><span class="n">res</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                    Model Comparison                    
========================================================
                                   OLS              2SLS
--------------------------------------------------------
Dep. Variable            np.log(packs)     np.log(packs)
Estimator                          OLS           IV-2SLS
No. Observations                    96                96
Cov. Est.                    clustered         clustered
R-squared                       0.9675            0.9659
Adj. R-squared                  0.9313            0.9279
F-statistic                 -3.086e+17          2.66e+16
P-value (F-stat)                1.0000            0.0000
==================     ===============   ===============
Intercept                       8.3597            9.4924
                              (484.80)          (360.24)
C(year)[T.1995]                -0.0885           -0.0328
                                                        
C(state)[T.AR]                  0.1686            0.1770
                              (3.7460)          (3.3338)
C(state)[T.AZ]                 -0.1280           -0.0899
                             (-49.908)         (-6.8132)
C(state)[T.CA]                 -0.3320           -0.2781
                             (-9.0787)         (-12.996)
C(state)[T.CO]                 -0.2591           -0.2479
                             (-345.21)         (-27.625)
C(state)[T.CT]                 -0.1042           -0.0171
                             (-3.8346)         (-0.8720)
C(state)[T.DE]                  0.0900            0.1110
                              (3.3042)          (3.8105)
C(state)[T.FL]                  0.0325            0.0762
                              (1.9171)          (5.3596)
C(state)[T.GA]                 -0.0691           -0.0695
                             (-2.4532)         (-2.7706)
C(state)[T.IA]                 -0.0143            0.0120
                             (-0.2380)          (0.1629)
C(state)[T.ID]                 -0.1422           -0.1272
                             (-8.4099)         (-16.597)
C(state)[T.IL]                 -0.0766           -0.0339
                             (-10.214)         (-4.1912)
C(state)[T.IN]                  0.1231            0.1198
                              (2.0976)          (1.9609)
C(state)[T.KS]                 -0.1073           -0.0910
                             (-4.8883)         (-2.9884)
C(state)[T.KY]                  0.3803            0.3525
                              (6.6476)          (5.5906)
C(state)[T.LA]                  0.1174            0.1315
                              (12.648)          (12.664)
C(state)[T.MA]                 -0.1055           -0.0403
                             (-10.125)         (-5.8826)
C(state)[T.MD]                 -0.2572           -0.2322
                             (-62.161)         (-9.7376)
C(state)[T.ME]                  0.1682            0.2008
                              (3.7655)          (3.5011)
C(state)[T.MI]                  0.0652            0.1268
                              (1.4699)          (1.7009)
C(state)[T.MN]                 -0.0051            0.0568
                             (-0.1580)          (1.1595)
C(state)[T.MO]                  0.0600            0.0640
                              (1.2722)          (1.3454)
C(state)[T.MS]                  0.1472            0.1501
                              (6.1510)          (5.5267)
C(state)[T.MT]                 -0.1469           -0.1522
                             (-26.043)         (-28.250)
C(state)[T.NC]                  0.0609            0.0396
                              (7.1838)          (2.0655)
C(state)[T.ND]                 -0.0595           -0.0311
                             (-1.9467)         (-0.7787)
C(state)[T.NE]                 -0.1002           -0.0741
                             (-3.4255)         (-1.9765)
C(state)[T.NH]                  0.3374            0.3504
                              (14.331)          (11.114)
C(state)[T.NJ]                 -0.1458           -0.0873
                             (-11.223)         (-1429.3)
C(state)[T.NM]                 -0.2981           -0.2858
                             (-28.465)         (-71.049)
C(state)[T.NV]                  0.1219            0.1789
                              (3.2128)          (6.9075)
C(state)[T.NY]                 -0.1369           -0.0719
                             (-5.8164)         (-22.256)
C(state)[T.OH]                  0.0196            0.0325
                              (0.5379)          (0.8088)
C(state)[T.OK]                  0.0841            0.0946
                              (1.6659)          (1.7572)
C(state)[T.OR]                 -0.0380           -0.0153
                             (-0.7768)         (-0.2269)
C(state)[T.PA]                 -0.0333           -0.0031
                             (-16.905)         (-4.8401)
C(state)[T.RI]                  0.0900            0.1394
                              (1.3979)          (1.5136)
C(state)[T.SC]                 -0.0037           -0.0212
                             (-0.1369)         (-0.6345)
C(state)[T.SD]                 -0.0695           -0.0675
                             (-1.1052)         (-0.9488)
C(state)[T.TN]                  0.1373            0.1473
                              (3.2924)          (3.1340)
C(state)[T.TX]                 -0.0964           -0.0579
                             (-3.8212)         (-4.2560)
C(state)[T.UT]                 -0.5127           -0.4899
                             (-16.600)         (-17.776)
C(state)[T.VA]                 -0.0628           -0.0559
                             (-1.7642)         (-1.1875)
C(state)[T.VT]                  0.2053            0.2209
                              (5.3662)          (4.7267)
C(state)[T.WA]                 -0.0777            0.0064
                             (-6.5481)          (6.0151)
C(state)[T.WI]                  0.0260            0.0741
                              (0.5209)          (1.2569)
C(state)[T.WV]                  0.1490            0.1576
                              (2.4854)          (2.7097)
C(state)[T.WY]                 -0.0150           -0.0169
                             (-0.2771)         (-0.2858)
np.log(rincome)                 0.4974            0.4434
                                             (7.068e+06)
np.log(rprice)                 -1.0560           -1.2793
                                             (-1.47e+08)
==================== ================= =================
Instruments                                         taxs
--------------------------------------------------------

T-stats reported in parentheses
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/arthurturrell/opt/anaconda3/envs/codeforecon/lib/python3.8/site-packages/linearmodels/iv/results.py:189: RuntimeWarning: invalid value encountered in sqrt
  std_errors = sqrt(diag(self.cov))
</pre></div>
</div>
</div>
</div>
<p>Once we take into account the fact that the real price is endogeneous to (realised) demand, we find that its coefficient is more negative; i.e. an increase in the real price of cigarettes creates a bigger fall in number of packs bought.</p>
</div>
<div class="section" id="logit-probit-and-generalised-linear-models">
<h2><span class="section-number">3.9. </span>Logit, probit, and generalised linear models<a class="headerlink" href="#logit-probit-and-generalised-linear-models" title="Permalink to this headline">¶</a></h2>
<div class="section" id="logit">
<h3><span class="section-number">3.9.1. </span>Logit<a class="headerlink" href="#logit" title="Permalink to this headline">¶</a></h3>
<p>A logistical regression, aka a logit, is a statistical method for a best-fit line between a regressors <span class="math notranslate nohighlight">\(X\)</span> and an outcome varibale <span class="math notranslate nohighlight">\(y\)</span> that takes on values in <span class="math notranslate nohighlight">\((0, 1)\)</span>.</p>
<p>The function that we’re assuming links the regressors and the outcome has a few different names but the most common is the sigmoid function or the logistic function. The data generating process is assumed to be</p>
<div class="math notranslate nohighlight">
\[
{\displaystyle \mathbb{P}(Y=1\mid X) = \frac{1}{1 + e^{-X'\beta}}}
\]</div>
<p>we can also write this as <span class="math notranslate nohighlight">\(\ln\left(\frac{p}{p-1}\right) = \beta_0 + \sum_i \beta_i x_i\)</span> to get a ‘log-odds’ relationship. The coefficients from a logit model do not have the same interpration as in an OLS estimation, and you can see this from the fact that <span class="math notranslate nohighlight">\(\partial y/\partial x_i \neq \beta_i\)</span> for logit. Of course, you can work out what the partial derivative is for yourself but most packages offer a convenient way to quickly recover the marginal effects.</p>
<p>Logit models are available in <strong>scikit-learn</strong> and <strong>statsmodels</strong> but bear in mind that the <strong>scikit-learn</strong> logit model is, ermm, extremely courageous in that regularisation is applied by default. If you don’t know what that means, don’t worry, but it’s probably best to stick with <strong>statsmodels</strong> as we will do in this example.</p>
<p>We will predict a target <code class="docutils literal notranslate"><span class="pre">GRADE</span></code>, representing whether a grade improved or not, based on some regressors including participation in a programme.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the data from Spector and Mazzeo (1980)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">spector</span><span class="o">.</span><span class="n">load_pandas</span><span class="p">()</span><span class="o">.</span><span class="n">data</span>
<span class="c1"># Look at info on data</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sm</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">spector</span><span class="o">.</span><span class="n">NOTE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>::

    Number of Observations - 32

    Number of Variables - 4

    Variable name definitions::

        Grade - binary variable indicating whether or not a student&#39;s grade
                improved.  1 indicates an improvement.
        TUCE  - Test score on economics test
        PSI   - participation in program
        GPA   - Student&#39;s grade point average
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">res_logit</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">logit</span><span class="p">(</span><span class="s1">&#39;GRADE ~ GPA + TUCE + PSI&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res_logit</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.402801
         Iterations 7
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                  GRADE   No. Observations:                   32
Model:                          Logit   Df Residuals:                       28
Method:                           MLE   Df Model:                            3
Date:                Sun, 28 Feb 2021   Pseudo R-squ.:                  0.3740
Time:                        13:26:59   Log-Likelihood:                -12.890
converged:                       True   LL-Null:                       -20.592
Covariance Type:            nonrobust   LLR p-value:                  0.001502
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept    -13.0213      4.931     -2.641      0.008     -22.687      -3.356
GPA            2.8261      1.263      2.238      0.025       0.351       5.301
TUCE           0.0952      0.142      0.672      0.501      -0.182       0.373
PSI            2.3787      1.065      2.234      0.025       0.292       4.465
==============================================================================
</pre></div>
</div>
</div>
</div>
<p>So, did participation (<code class="docutils literal notranslate"><span class="pre">PSI</span></code>) help increase a grade? Yes. But we need to check the marginal effect to say exactly how much. We’ll use <code class="docutils literal notranslate"><span class="pre">get_margeff</span></code> to do this, we’d like the <span class="math notranslate nohighlight">\(dy/dx\)</span> effect, and we’ll take it at the mean of each regressor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">marg_effect</span> <span class="o">=</span> <span class="n">res_logit</span><span class="o">.</span><span class="n">get_margeff</span><span class="p">(</span><span class="n">at</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;dydx&#39;</span><span class="p">)</span>
<span class="n">marg_effect</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<caption>Logit Marginal Effects</caption>
<tr>
  <th>Dep. Variable:</th> <td>GRADE</td>
</tr>
<tr>
  <th>Method:</th>        <td>dydx</td> 
</tr>
<tr>
  <th>At:</th>            <td>mean</td> 
</tr>
</table>
<table class="simpletable">
<tr>
    <th></th>      <th>dy/dx</th>    <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>GPA</th>  <td>    0.5339</td> <td>    0.237</td> <td>    2.252</td> <td> 0.024</td> <td>    0.069</td> <td>    0.998</td>
</tr>
<tr>
  <th>TUCE</th> <td>    0.0180</td> <td>    0.026</td> <td>    0.685</td> <td> 0.493</td> <td>   -0.033</td> <td>    0.069</td>
</tr>
<tr>
  <th>PSI</th>  <td>    0.4493</td> <td>    0.197</td> <td>    2.284</td> <td> 0.022</td> <td>    0.064</td> <td>    0.835</td>
</tr>
</table></div></div>
</div>
<p>So participation gives almost half a grade increase.</p>
</div>
<div class="section" id="probit">
<h3><span class="section-number">3.9.2. </span>Probit<a class="headerlink" href="#probit" title="Permalink to this headline">¶</a></h3>
<p>Probit is very similar to logit: it’s a statistical method for a best-fit line between regressors <span class="math notranslate nohighlight">\(X\)</span> and an outcome varibale <span class="math notranslate nohighlight">\(y\)</span> that takes on values in <span class="math notranslate nohighlight">\((0, 1)\)</span>. And, just like with logit, the function that we’re assuming links the regressors and the outcome has a few different names!</p>
<p>The data generating process is assumed to be</p>
<div class="math notranslate nohighlight">
\[
{\displaystyle \mathbb{P}(Y=1\mid X)=\Phi (X^{T}\beta )}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
{\displaystyle \Phi (x)={\frac {1}{\sqrt {2\pi }}}\int _{-\infty }^{x}e^{-{\frac {y^{2}}{2}}}dy.}
\]</div>
<p>is the cumulative standard normal (aka Gaussian) distribution. The coefficients from a probit model do not have the same interpration as in an OLS estimation, and you can see this from the fact that <span class="math notranslate nohighlight">\(\partial y/\partial x_i \neq \beta_i\)</span> for probit. And, just as with logit, although you can derive the marginal effects, most packages offer a convenient way to quickly recover them.</p>
<p>We can re-use our previous example of predicting a target <code class="docutils literal notranslate"><span class="pre">GRADE</span></code>, representing whether a grade improved or not, based on some regressors including participation (PSI) in a programme.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">res_probit</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">probit</span><span class="p">(</span><span class="s1">&#39;GRADE ~ GPA + TUCE + PSI&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res_probit</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.400588
         Iterations 6
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                          Probit Regression Results                           
==============================================================================
Dep. Variable:                  GRADE   No. Observations:                   32
Model:                         Probit   Df Residuals:                       28
Method:                           MLE   Df Model:                            3
Date:                Sun, 28 Feb 2021   Pseudo R-squ.:                  0.3775
Time:                        13:26:59   Log-Likelihood:                -12.819
converged:                       True   LL-Null:                       -20.592
Covariance Type:            nonrobust   LLR p-value:                  0.001405
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -7.4523      2.542     -2.931      0.003     -12.435      -2.469
GPA            1.6258      0.694      2.343      0.019       0.266       2.986
TUCE           0.0517      0.084      0.617      0.537      -0.113       0.216
PSI            1.4263      0.595      2.397      0.017       0.260       2.593
==============================================================================
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p_marg_effect</span> <span class="o">=</span> <span class="n">res_probit</span><span class="o">.</span><span class="n">get_margeff</span><span class="p">(</span><span class="n">at</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;dydx&#39;</span><span class="p">)</span>
<span class="n">p_marg_effect</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<caption>Probit Marginal Effects</caption>
<tr>
  <th>Dep. Variable:</th> <td>GRADE</td>
</tr>
<tr>
  <th>Method:</th>        <td>dydx</td> 
</tr>
<tr>
  <th>At:</th>            <td>mean</td> 
</tr>
</table>
<table class="simpletable">
<tr>
    <th></th>      <th>dy/dx</th>    <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>GPA</th>  <td>    0.5333</td> <td>    0.232</td> <td>    2.294</td> <td> 0.022</td> <td>    0.078</td> <td>    0.989</td>
</tr>
<tr>
  <th>TUCE</th> <td>    0.0170</td> <td>    0.027</td> <td>    0.626</td> <td> 0.531</td> <td>   -0.036</td> <td>    0.070</td>
</tr>
<tr>
  <th>PSI</th>  <td>    0.4679</td> <td>    0.188</td> <td>    2.494</td> <td> 0.013</td> <td>    0.100</td> <td>    0.836</td>
</tr>
</table></div></div>
</div>
<p>It’s no coincidence that we find very similar results here because the two functions we’re using don’t actually look all that different:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">st</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">support</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">support</span><span class="p">,</span> <span class="n">st</span><span class="o">.</span><span class="n">logistic</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">support</span><span class="p">),</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Logistic&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">support</span><span class="p">,</span> <span class="n">st</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">support</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Probit&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/econmt-regression_93_0.png" src="_images/econmt-regression_93_0.png" />
</div>
</div>
<p>What difference there is, is that logistic regression puts more weight into the tails of the distribution. Arguably, logit is easier to interpret too. With logistic regression, a one unit change in <span class="math notranslate nohighlight">\(x_i\)</span> is associated with a <span class="math notranslate nohighlight">\(\beta_i\)</span> change in the log odds of a 1 outcome or, alternatively, an <span class="math notranslate nohighlight">\(e^{\beta_i}\)</span>-fold change in the odds, all else being equal. With a probit, this is a change of <span class="math notranslate nohighlight">\(\beta_i z\)</span> for <span class="math notranslate nohighlight">\(z\)</span> a normalised variable that you’d have to convert into a predicted probability using the normal CDF.</p>
</div>
<div class="section" id="generalised-linear-models">
<h3><span class="section-number">3.9.3. </span>Generalised linear models<a class="headerlink" href="#generalised-linear-models" title="Permalink to this headline">¶</a></h3>
<p>Logit and probit (and OLS for that matter) as special cases of a class of models such that <span class="math notranslate nohighlight">\(g\)</span> is a ‘link’ function connects a function of regressors to the output, and <span class="math notranslate nohighlight">\(\mu\)</span> is the mean of a conditional response distribution at a given point in the space of regressors. When <span class="math notranslate nohighlight">\(g(\mu) = X'\beta\)</span>, we just get regular OLS. When it’s logit, we have</p>
<div class="math notranslate nohighlight">
\[
{\displaystyle \mu= \mathbb{E}(Y\mid X=x) =g^{-1}(X'\beta)= \frac{1}{1 + e^{-X'\beta}}.}
\]</div>
<p>But as well as the ones we’ve seen, there are many possible link functions one can use via the catch-all <code class="docutils literal notranslate"><span class="pre">glm</span></code> function. These come in different ‘families’ of distributions, with the default for the binomial family being logit. So, running <code class="docutils literal notranslate"><span class="pre">smf.glm('GRADE</span> <span class="pre">~</span> <span class="pre">GPA</span> <span class="pre">+</span> <span class="pre">TUCE</span> <span class="pre">+</span> <span class="pre">PSI',</span> <span class="pre">data=df,</span> <span class="pre">family=sm.families.Binomial()).fit()</span></code> will produce exactly the same as we got both using the <code class="docutils literal notranslate"><span class="pre">logit</span></code> function. For more on the families of distributions and possible link functions, see the <a class="reference external" href="https://www.statsmodels.org/stable/glm.html">relevant part</a> of the <strong>statsmodels</strong> documentation.</p>
</div>
</div>
<div class="section" id="quantile-regression">
<h2><span class="section-number">3.10. </span>Quantile regression<a class="headerlink" href="#quantile-regression" title="Permalink to this headline">¶</a></h2>
<p>Quantile regression estimates the conditional quantiles of a response variable. In some cases, it can be more robust to outliers and, in the case of the <span class="math notranslate nohighlight">\(q=0.5\)</span> quantile it is equivalent LAD (Least Absolute Deviation) regression. Let’s look at an example of quantile regression in action, lifted direct from the <strong>statsmodels</strong> <a class="reference external" href="https://www.statsmodels.org/dev/examples/notebooks/generated/quantile_regression.html">documentation</a> and based on a Journal of Economic Perspectives paper by Koenker and Hallock.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">engel</span><span class="o">.</span><span class="n">load_pandas</span><span class="p">()</span><span class="o">.</span><span class="n">data</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>income</th>
      <th>foodexp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>420.157651</td>
      <td>255.839425</td>
    </tr>
    <tr>
      <th>1</th>
      <td>541.411707</td>
      <td>310.958667</td>
    </tr>
    <tr>
      <th>2</th>
      <td>901.157457</td>
      <td>485.680014</td>
    </tr>
    <tr>
      <th>3</th>
      <td>639.080229</td>
      <td>402.997356</td>
    </tr>
    <tr>
      <th>4</th>
      <td>750.875606</td>
      <td>495.560775</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>What we have here are two sets of related data. Let’s perform several quantile regressions from 0.1 to 0.9 in steps of 0.1</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mod</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">quantreg</span><span class="p">(</span><span class="s1">&#39;foodexp ~ income&#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
<span class="n">quantiles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">q_results</span> <span class="o">=</span> <span class="p">[</span><span class="n">mod</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">quantiles</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>The <span class="math notranslate nohighlight">\(q=0.5\)</span> entry will be at the <code class="docutils literal notranslate"><span class="pre">4</span></code> index; let’s take a look at it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">q_results</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                         QuantReg Regression Results                          
==============================================================================
Dep. Variable:                foodexp   Pseudo R-squared:               0.6206
Model:                       QuantReg   Bandwidth:                       64.51
Method:                 Least Squares   Sparsity:                        209.3
Date:                Sun, 28 Feb 2021   No. Observations:                  235
Time:                        13:27:00   Df Residuals:                      233
                                        Df Model:                            1
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     81.4823     14.634      5.568      0.000      52.649     110.315
income         0.5602      0.013     42.516      0.000       0.534       0.586
==============================================================================

The condition number is large, 2.38e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>Let’s take a look at the results for all of the regressions <em>and</em> let’s add in OLS for comparison:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ols_res</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;foodexp ~ income&#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="n">get_y</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">income</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">df</span><span class="o">.</span><span class="n">income</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">50</span><span class="p">)</span>
<span class="c1"># Just to make the plot clearer</span>
<span class="n">x_max</span> <span class="o">=</span> <span class="mi">3000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">x</span><span class="o">&lt;</span><span class="n">x_max</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">df</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;income&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;foodexp&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">res</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">q_results</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">get_y</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;Intercept&#39;</span><span class="p">],</span> <span class="n">res</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;income&#39;</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$q=</span><span class="si">{</span><span class="n">quantiles</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s1">1.1f</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">()))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">get_y</span><span class="p">(</span><span class="n">ols_res</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;Intercept&#39;</span><span class="p">],</span> <span class="n">ols_res</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;income&#39;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;OLS&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x_max</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/econmt-regression_103_0.png" src="_images/econmt-regression_103_0.png" />
</div>
</div>
<p>This chart shows very clearly how quantile regression differs from OLS. The line fitted by OLS is trying to be all things to all points whereas the line fitted by quantile regression is focused only on its quantile. You can also see how points far from the median (not all shown) may be having a large influence on the OLS line.</p>
</div>
<div class="section" id="regression-plots">
<h2><span class="section-number">3.11. </span>Regression plots<a class="headerlink" href="#regression-plots" title="Permalink to this headline">¶</a></h2>
<p><strong>statsmodels</strong> has a number of built-in plotting methods to help you understand how well your regression is capturing the relationships you’re looking for. Let’s see a few examples of these using <strong>statsmodels</strong> built-in Statewide Crime data set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">crime_data</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">statecrime</span><span class="o">.</span><span class="n">load_pandas</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sm</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">statecrime</span><span class="o">.</span><span class="n">NOTE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>::

    Number of observations: 51
    Number of variables: 8
    Variable name definitions:

    state
        All 50 states plus DC.
    violent
        Rate of violent crimes / 100,000 population. Includes murder, forcible
        rape, robbery, and aggravated assault. Numbers for Illinois and
        Minnesota do not include forcible rapes. Footnote included with the
        American Statistical Abstract table reads:
        &quot;The data collection methodology for the offense of forcible
        rape used by the Illinois and the Minnesota state Uniform Crime
        Reporting (UCR) Programs (with the exception of Rockford, Illinois,
        and Minneapolis and St. Paul, Minnesota) does not comply with
        national UCR guidelines. Consequently, their state figures for
        forcible rape and violent crime (of which forcible rape is a part)
        are not published in this table.&quot;
    murder
        Rate of murders / 100,000 population.
    hs_grad
        Percent of population having graduated from high school or higher.
    poverty
        % of individuals below the poverty line
    white
        Percent of population that is one race - white only. From 2009 American
        Community Survey
    single
        Calculated from 2009 1-year American Community Survey obtained obtained
        from Census. Variable is Male householder, no wife present, family
        household combined with Female householder, no husband present, family
        household, divided by the total number of Family households.
    urban
        % of population in Urbanized Areas as of 2010 Census. Urbanized
        Areas are area of 50,000 or more people.
</pre></div>
</div>
</div>
</div>
<p>First, let’s look at a Q-Q plot to get a sense of how the variables are distributed. This uses <strong>scipy</strong>’s stats module. The default distribution is normal but you can use any that <strong>scipy</strong> supports.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">st</span><span class="o">.</span><span class="n">probplot</span><span class="p">(</span><span class="n">crime_data</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;murder&#39;</span><span class="p">],</span> <span class="n">dist</span><span class="o">=</span><span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="n">plt</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/econmt-regression_108_0.png" src="_images/econmt-regression_108_0.png" />
</div>
</div>
<p>Clearly, this is not quite normal and there are some serious outliers in the tails.</p>
<p>Let’s run take a look at the unconditional relationship we’re interested in: how murder depends on high school graduation. We’ll use <a class="reference external" href="https://plotnine.readthedocs.io/en/stable/index.html"><strong>plotnine</strong></a>’s <code class="docutils literal notranslate"><span class="pre">geom_smooth</span></code> to do this but bear in mind it will only run a linear model of <code class="docutils literal notranslate"><span class="pre">'murder</span> <span class="pre">~</span> <span class="pre">hs_grad'</span></code> and ignore the other covariates.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">plotnine</span> <span class="kn">import</span> <span class="o">*</span>

<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">crime_data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;murder&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;hs_grad&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">geom_point</span><span class="p">()</span>
    <span class="o">+</span> <span class="n">geom_smooth</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;lm&#39;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/econmt-regression_110_0.png" src="_images/econmt-regression_110_0.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ggplot: (8784467115316)&gt;
</pre></div>
</div>
</div>
</div>
<p>We can take into account those other factors by using a partial regression plot that asks what does <span class="math notranslate nohighlight">\(\mathbb{E}(y|X)\)</span> look like as a function of <span class="math notranslate nohighlight">\(\mathbb{E}(x_i|X)\)</span>? (Use <code class="docutils literal notranslate"><span class="pre">obs_labels=False</span></code> to remove data point labels.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">rc_context</span><span class="p">({</span><span class="s1">&#39;font.size&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">}):</span>
    <span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">plot_partregress</span><span class="p">(</span><span class="n">endog</span><span class="o">=</span><span class="s1">&#39;murder&#39;</span><span class="p">,</span> <span class="n">exog_i</span><span class="o">=</span><span class="s1">&#39;hs_grad&#39;</span><span class="p">,</span>
                                <span class="n">exog_others</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;urban&#39;</span><span class="p">,</span> <span class="s1">&#39;poverty&#39;</span><span class="p">,</span> <span class="s1">&#39;single&#39;</span><span class="p">],</span>
                                <span class="n">data</span><span class="o">=</span><span class="n">crime_data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">obs_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/econmt-regression_112_0.png" src="_images/econmt-regression_112_0.png" />
</div>
</div>
<p>At this point, the results of the regression are useful context.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_crime</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;murder ~ hs_grad + urban + poverty + single&#39;</span><span class="p">,</span>
                  <span class="n">data</span><span class="o">=</span><span class="n">crime_data</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_crime</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                 murder   R-squared:                       0.813
Model:                            OLS   Adj. R-squared:                  0.797
Method:                 Least Squares   F-statistic:                     50.08
Date:                Sun, 28 Feb 2021   Prob (F-statistic):           3.42e-16
Time:                        13:27:01   Log-Likelihood:                -95.050
No. Observations:                  51   AIC:                             200.1
Df Residuals:                      46   BIC:                             209.8
Df Model:                           4                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept    -44.1024     12.086     -3.649      0.001     -68.430     -19.774
hs_grad        0.3059      0.117      2.611      0.012       0.070       0.542
urban          0.0109      0.015      0.707      0.483      -0.020       0.042
poverty        0.4121      0.140      2.939      0.005       0.130       0.694
single         0.6374      0.070      9.065      0.000       0.496       0.779
==============================================================================
Omnibus:                        1.618   Durbin-Watson:                   2.507
Prob(Omnibus):                  0.445   Jarque-Bera (JB):                0.831
Skew:                          -0.220   Prob(JB):                        0.660
Kurtosis:                       3.445   Cond. No.                     5.80e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 5.8e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>Putting the multicollinearity problems to one side, we see that the relationship shown in the partial regression plot is also implied by the coefficient on <code class="docutils literal notranslate"><span class="pre">hs_grad</span></code> in the regression table.</p>
<p>We can also look at an in-depth summary of one exogenous regressor and its relationship to the outcome variable. Each of these types of regression diagnostic are available individually, or for all regressors at once, too. The first panel is the chart we did with <strong>plotnine</strong> rendered differently (and, one could argue, more informatively). Most of the plots below are self-explanatory except for the third one,  the CCPR (Component-Component plus Residual) plot. This provides a way to judge the effect of one regressor on the response variable by taking into account the effects of the other independent variables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>

<span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">plot_regress_exog</span><span class="p">(</span><span class="n">results_crime</span><span class="p">,</span> <span class="s1">&#39;hs_grad&#39;</span><span class="p">,</span> <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/econmt-regression_117_0.png" src="_images/econmt-regression_117_0.png" />
</div>
</div>
<p><strong>statsmodels</strong> can also produce influence plots of the ‘externally studentised’ residuals vs. the leverage of each observation as measured by the so-called hat matrix <span class="math notranslate nohighlight">\(X(X^{\;\prime}X)^{-1}X^{\;\prime}\)</span> (because it puts the ‘hat’ on <span class="math notranslate nohighlight">\(y\)</span>). Externally studentised residuals are residuals that are scaled by their standard deviation. High leverage points could exert an undue influence over the regression line, but only if the predicted <span class="math notranslate nohighlight">\(y\)</span> values of a regression that was fit with them excluded was quite different. In the example below, DC is having a big influence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">rc_context</span><span class="p">({</span><span class="s1">&#39;font.size&#39;</span><span class="p">:</span> <span class="mi">6</span><span class="p">}):</span>
    <span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">influence_plot</span><span class="p">(</span><span class="n">results_crime</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/econmt-regression_120_0.png" src="_images/econmt-regression_120_0.png" />
</div>
</div>
<p>Finally, it’s nice to be able to see plots of our coefficients along with their standard errors. There isn’t a built-in <strong>statsmodels</strong> option for this, but happily it’s easy to extract the results of regressions in a sensible format. Using the <code class="docutils literal notranslate"><span class="pre">results</span></code> object from earlier, and excluding the intercept, we can get the coefficients from <code class="docutils literal notranslate"><span class="pre">results.params[1:]</span></code> and the associated errors from <code class="docutils literal notranslate"><span class="pre">results.bse[1:]</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Put the results into a dataframe with Name, Coefficient, Error</span>
<span class="n">res_df</span> <span class="o">=</span> <span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_crime</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">results_crime</span><span class="o">.</span><span class="n">bse</span><span class="p">[</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
            <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="s1">&#39;Name&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;Coefficient&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;Error&#39;</span><span class="p">}))</span>
<span class="c1"># Plot the coefficient values and their errors</span>
<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">res_df</span><span class="p">)</span> <span class="o">+</span>
    <span class="n">geom_point</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="s2">&quot;Name&quot;</span><span class="p">,</span> <span class="s2">&quot;Coefficient&quot;</span><span class="p">))</span> <span class="o">+</span>
    <span class="n">geom_errorbar</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;Name&quot;</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="s2">&quot;Coefficient-Error&quot;</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="s2">&quot;Coefficient+Error&quot;</span><span class="p">))</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/econmt-regression_122_0.png" src="_images/econmt-regression_122_0.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ggplot: (8784484096359)&gt;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="specification-curve-analysis">
<h2><span class="section-number">3.12. </span>Specification curve analysis<a class="headerlink" href="#specification-curve-analysis" title="Permalink to this headline">¶</a></h2>
<p>When specifying a model, modellers have many options. These can be informed by field intelligence, priors, and even misguided attempts to find a significant result. Even with the best of intentions, research teams can reach entirely different conclusions using the same, or similar, data because of different choices made in preparing data or in modelling it.</p>
<p>There’s formal evidence that researchers really do make different decisions; one study <span id="id3">[<a class="reference internal" href="zreferences.html#id8"><span>9</span></a>]</span> gave the same research question - whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players - to 29 different teams. From the abstract of that paper:</p>
<blockquote>
<div><p>Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69%) found a statistically significant positive effect, and 9 teams (31%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts’ prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability.</p>
</div></blockquote>
<p>So not only were different decisions made, there seems to be no clearly identifiable reason for them. There is usually scope for reasonable alternative model specifications when estimating coefficients, and those coefficients will vary with those specifications.</p>
<p>Specification curve analysis <span id="id4">[<a class="reference internal" href="zreferences.html#id9"><span>10</span></a>]</span> looks for a more exhaustive way of trying out alternative specifications. The three steps of specification curve analysis are:</p>
<ol class="simple">
<li><p>identifying the set of theoretically justified, statistically valid, and non-redundant analytic specifications;</p></li>
<li><p>displaying alternative results graphically, allowing the identification of decisions producing different results; and</p></li>
<li><p>conducting statistical tests to determine whether as a whole results are inconsistent with the null hypothesis.</p></li>
</ol>
<p>For a good example of specification curve analysis in action, see this recent Nature Human Behaviour paper <span id="id5">[<a class="reference internal" href="zreferences.html#id10"><span>11</span></a>]</span> on the association between adolescent well-being and the use of digital technology.</p>
<p>We’ll use the <a class="reference external" href="https://specification-curve.readthedocs.io/en/latest/readme.html"><strong>specification curve analysis</strong></a> package to do the first two, which you can install with <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">specification_curve</span></code> (full disclosure: I wrote this package). To demonstrate the full functionality, we’ll create a second, alternative ‘hp’ that is a transformed version of the original.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mpg</span><span class="p">[</span><span class="s1">&#39;hp_boxcox&#39;</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">boxcox</span><span class="p">(</span><span class="n">mpg</span><span class="p">[</span><span class="s1">&#39;hp&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s create a specification curve. We need to specify the data, the different outcome variables we’d like to try, <code class="docutils literal notranslate"><span class="pre">y_endog</span></code>; the different possible versions of the main regressor of interest, <code class="docutils literal notranslate"><span class="pre">x_exog</span></code>; the possible controls, <code class="docutils literal notranslate"><span class="pre">controls</span></code>; any controls that should always be included, <code class="docutils literal notranslate"><span class="pre">always_include</span></code>; and any categorical variables to include class-by-class, <code class="docutils literal notranslate"><span class="pre">cat_expand</span></code>. Some of these accept lists of variables as well as single reggressors. The point estimates that have confidence intervals which include zero are coloured in grey, instead of blue. There is also an <code class="docutils literal notranslate"><span class="pre">exclu_grps</span></code> option to exclude certain combinations of regressors, and you can pass alternative estimators to fit, for example <code class="docutils literal notranslate"><span class="pre">fit(estimator=sm.Logit)</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">specification_curve</span> <span class="kn">import</span> <span class="n">specification_curve</span> <span class="k">as</span> <span class="n">specy</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">specy</span><span class="o">.</span><span class="n">SpecificationCurve</span><span class="p">(</span><span class="n">mpg</span><span class="p">,</span>
                              <span class="n">y_endog</span><span class="o">=</span><span class="s1">&#39;mpg&#39;</span><span class="p">,</span>
                              <span class="n">x_exog</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;lnhp&#39;</span><span class="p">,</span> <span class="s1">&#39;hp_boxcox&#39;</span><span class="p">],</span>
                              <span class="n">controls</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;drat&#39;</span><span class="p">,</span> <span class="s1">&#39;qsec&#39;</span><span class="p">,</span> <span class="s1">&#39;cyl&#39;</span><span class="p">,</span> <span class="s1">&#39;gear&#39;</span><span class="p">],</span>
                              <span class="n">always_include</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;gear&#39;</span><span class="p">],</span>
                              <span class="n">cat_expand</span><span class="o">=</span><span class="s1">&#39;cyl&#39;</span><span class="p">)</span>
<span class="n">sc</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">sc</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fit complete
</pre></div>
</div>
<img alt="_images/econmt-regression_126_1.png" src="_images/econmt-regression_126_1.png" />
</div>
</div>
</div>
<div class="section" id="review">
<h2><span class="section-number">3.13. </span>Review<a class="headerlink" href="#review" title="Permalink to this headline">¶</a></h2>
<p>In this very short introduction to regression with code, you should have learned how to:</p>
<ul class="simple">
<li><p>✅ perform linear OLS regressions with code;</p></li>
<li><p>✅ add fixed effects/categorical variables to regressions;</p></li>
<li><p>✅ use different standard errors;</p></li>
<li><p>✅ use models with transformed regressors;</p></li>
<li><p>✅ use the formula or array APIs for <strong>statsmodels</strong> and <strong>linearmodels</strong>;</p></li>
<li><p>✅ show the results from multiple models;</p></li>
<li><p>✅ perform IV regressions;</p></li>
<li><p>✅ perform GLM regressions; and</p></li>
<li><p>✅ use plots as a way to interrogate regression results.</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "codeforecon"
        },
        kernelOptions: {
            kernelName: "codeforecon",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'codeforecon'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="econmt-statistics.html" title="previous page"><span class="section-number">2. </span>Statistics</a>
    <a class='right-next' id="next-link" href="geo-intro.html" title="next page"><span class="section-number">1. </span>Intro to Geo-Spatial Analysis</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Arthur Turrell<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            This book is available under an MIT license.

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-189705534-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>